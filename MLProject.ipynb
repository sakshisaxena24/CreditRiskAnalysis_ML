{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60302b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as nm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import xgboost as xgb\n",
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4829fd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458908</th>\n",
       "      <td>ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458909</th>\n",
       "      <td>ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458910</th>\n",
       "      <td>ffff9984b999fccb2b6127635ed0736dda94e544e67e02...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458911</th>\n",
       "      <td>ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458912</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458913 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              customer_ID  target\n",
       "0       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...       0\n",
       "1       00000fd6641609c6ece5454664794f0340ad84dddce9a2...       0\n",
       "2       00001b22f846c82c51f6e3958ccd81970162bae8b007e8...       0\n",
       "3       000041bdba6ecadd89a52d11886e8eaaec9325906c9723...       0\n",
       "4       00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...       0\n",
       "...                                                   ...     ...\n",
       "458908  ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...       0\n",
       "458909  ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...       0\n",
       "458910  ffff9984b999fccb2b6127635ed0736dda94e544e67e02...       0\n",
       "458911  ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...       1\n",
       "458912  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...       0\n",
       "\n",
       "[458913 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels= pd.read_csv(\"train_labels.csv\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49638d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_data=pd.DataFrame()\n",
    "data= pd.read_csv('train_data.csv',nrows=1000000)\n",
    "data[\"S_2\"]=pd.to_datetime(data[\"S_2\"])\n",
    "data[\"Year-Month\"] = data[\"S_2\"].dt.to_period('M')\n",
    "data1=data.groupby('customer_ID').apply(lambda x: x.sample(n=1)).reset_index(drop = True)\n",
    "final_data=pd.concat([final_data,pd.merge(labels,data1, how= 'inner', on='customer_ID')])\n",
    "\n",
    "\n",
    "\n",
    "final_data.to_csv(\"Final_data-Step3.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a93b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>...</th>\n",
       "      <th>D_137</th>\n",
       "      <th>D_138</th>\n",
       "      <th>D_139</th>\n",
       "      <th>D_140</th>\n",
       "      <th>D_141</th>\n",
       "      <th>D_142</th>\n",
       "      <th>D_143</th>\n",
       "      <th>D_144</th>\n",
       "      <th>D_145</th>\n",
       "      <th>Year-Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-03-09</td>\n",
       "      <td>0.938469</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.008724</td>\n",
       "      <td>1.006838</td>\n",
       "      <td>0.009228</td>\n",
       "      <td>0.124035</td>\n",
       "      <td>0.008771</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>2017-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>0.936665</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>1.000653</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.126750</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.005492</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>2017-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-05-28</td>\n",
       "      <td>0.954180</td>\n",
       "      <td>0.091505</td>\n",
       "      <td>0.021655</td>\n",
       "      <td>1.009672</td>\n",
       "      <td>0.006815</td>\n",
       "      <td>0.123977</td>\n",
       "      <td>0.007598</td>\n",
       "      <td>0.009423</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>2017-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-06-13</td>\n",
       "      <td>0.960384</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.013683</td>\n",
       "      <td>1.002700</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.117169</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006117</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>2017-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-07-16</td>\n",
       "      <td>0.947248</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.015193</td>\n",
       "      <td>1.000727</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.117325</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>0.004946</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>2017-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea...</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>0.444918</td>\n",
       "      <td>0.214272</td>\n",
       "      <td>0.509806</td>\n",
       "      <td>0.024078</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.386644</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.920215</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.006138</td>\n",
       "      <td>0.004346</td>\n",
       "      <td>2017-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea...</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.445380</td>\n",
       "      <td>0.182068</td>\n",
       "      <td>0.511427</td>\n",
       "      <td>0.025369</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.372405</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.933117</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007064</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>2018-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea...</td>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>0.439738</td>\n",
       "      <td>0.155119</td>\n",
       "      <td>0.497809</td>\n",
       "      <td>0.023311</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.389353</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>0.917929</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>2018-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0.449821</td>\n",
       "      <td>0.155417</td>\n",
       "      <td>0.501278</td>\n",
       "      <td>0.025809</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>0.367082</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.926053</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>2018-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>2e3fc76a68eeae6697b0a6b10f9288b7acaa678bbf99cd...</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>0.481917</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.182513</td>\n",
       "      <td>0.192125</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>0.393390</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.239441</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>2017-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              customer_ID        S_2  \\\n",
       "0       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-03-09   \n",
       "1       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-04-07   \n",
       "2       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-05-28   \n",
       "3       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-06-13   \n",
       "4       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-07-16   \n",
       "...                                                   ...        ...   \n",
       "999995  2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea... 2017-12-01   \n",
       "999996  2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea... 2018-01-01   \n",
       "999997  2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea... 2018-02-28   \n",
       "999998  2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea... 2018-03-31   \n",
       "999999  2e3fc76a68eeae6697b0a6b10f9288b7acaa678bbf99cd... 2017-03-29   \n",
       "\n",
       "             P_2      D_39       B_1       B_2       R_1       S_3      D_41  \\\n",
       "0       0.938469  0.001733  0.008724  1.006838  0.009228  0.124035  0.008771   \n",
       "1       0.936665  0.005775  0.004923  1.000653  0.006151  0.126750  0.000798   \n",
       "2       0.954180  0.091505  0.021655  1.009672  0.006815  0.123977  0.007598   \n",
       "3       0.960384  0.002455  0.013683  1.002700  0.001373  0.117169  0.000685   \n",
       "4       0.947248  0.002483  0.015193  1.000727  0.007605  0.117325  0.004653   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "999995  0.444918  0.214272  0.509806  0.024078  0.001514  0.386644  0.001295   \n",
       "999996  0.445380  0.182068  0.511427  0.025369  0.008337  0.372405  0.002628   \n",
       "999997  0.439738  0.155119  0.497809  0.023311  0.000641  0.389353  0.009791   \n",
       "999998  0.449821  0.155417  0.501278  0.025809  0.006869  0.367082  0.001196   \n",
       "999999  0.481917  0.009878  0.182513  0.192125  0.005269  0.393390  0.004422   \n",
       "\n",
       "             B_3  ...  D_137  D_138     D_139     D_140     D_141  D_142  \\\n",
       "0       0.004709  ...    NaN    NaN  0.002427  0.003706  0.003818    NaN   \n",
       "1       0.002714  ...    NaN    NaN  0.003954  0.003167  0.005032    NaN   \n",
       "2       0.009423  ...    NaN    NaN  0.003269  0.007329  0.000427    NaN   \n",
       "3       0.005531  ...    NaN    NaN  0.006117  0.004516  0.003200    NaN   \n",
       "4       0.009312  ...    NaN    NaN  0.003671  0.004946  0.008889    NaN   \n",
       "...          ...  ...    ...    ...       ...       ...       ...    ...   \n",
       "999995  0.920215  ...    NaN    NaN  0.000700  0.004486  0.006241    NaN   \n",
       "999996  0.933117  ...    NaN    NaN  0.007064  0.005506  0.005020    NaN   \n",
       "999997  0.917929  ...    NaN    NaN  0.002184  0.004863  0.007474    NaN   \n",
       "999998  0.926053  ...    NaN    NaN  0.000752  0.004426  0.005118    NaN   \n",
       "999999  0.239441  ...    NaN    NaN  0.004562  0.003862  0.006811    NaN   \n",
       "\n",
       "           D_143     D_144     D_145  Year-Month  \n",
       "0       0.000569  0.000610  0.002674     2017-03  \n",
       "1       0.009576  0.005492  0.009217     2017-04  \n",
       "2       0.003429  0.006986  0.002603     2017-05  \n",
       "3       0.008419  0.006527  0.009600     2017-06  \n",
       "4       0.001670  0.008126  0.009827     2017-07  \n",
       "...          ...       ...       ...         ...  \n",
       "999995  0.001294  0.006138  0.004346     2017-12  \n",
       "999996  0.006667  0.004177  0.004452     2018-01  \n",
       "999997  0.004654  0.005166  0.000017     2018-02  \n",
       "999998  0.002986  0.000797  0.006316     2018-03  \n",
       "999999  0.002136  0.002043  0.008634     2017-03  \n",
       "\n",
       "[1000000 rows x 191 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627a2898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8546\n",
      "0.28902410484437163\n"
     ]
    }
   ],
   "source": [
    "new_Data= final_data[final_data[\"Year-Month\"]==\"2018-03\"]\n",
    "print(len(new_Data))\n",
    "Default_rate = nm.mean(new_Data['target'])\n",
    "print(Default_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23488b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82975, 192)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43a7a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.260825549864417"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm.mean(final_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95f5417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_ID', 'D_63', 'D_64']\n"
     ]
    }
   ],
   "source": [
    "dtypes=final_data.dtypes\n",
    "\n",
    "dtypelist = list()\n",
    "\n",
    "for col in dtypes.index:\n",
    "    if dtypes[col] == 'object':\n",
    "        dtypelist.append(col)\n",
    "        \n",
    "print(dtypelist)\n",
    "\n",
    "# final_data['D_63'].value_counts()\n",
    "# final_data['D_64'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd2ff219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15931200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = final_data.size\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b5e3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding\n",
    "\n",
    "one_hot = pd.get_dummies(final_data[['D_63', 'D_64']])\n",
    "\n",
    "# Combine the one hot encoded data with the original dataframe\n",
    "final_data = pd.concat([final_data, one_hot], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "final_data.drop(['D_63', 'D_64', 'customer_ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07fcf8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "for col in['B_30','B_38','D_114','D_116','D_117','D_120','D_126','D_66','D_68']:\n",
    "    col_dummies=pd.get_dummies(final_data[col],prefix=col)\n",
    "    final_data=pd.concat([final_data,col_dummies],axis=1)\n",
    "    final_data.drop(col,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff38169a-bc30-4eb3-a17f-01548ffab5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82975, 225)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65410665-47f4-4304-9d3e-7b81c1c8c7db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1275c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns',None)\n",
    "# final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "444cadf6-7450-4d2c-9850-8bb6da95b917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>...</th>\n",
       "      <th>D_126_1.0</th>\n",
       "      <th>D_66_0.0</th>\n",
       "      <th>D_66_1.0</th>\n",
       "      <th>D_68_0.0</th>\n",
       "      <th>D_68_1.0</th>\n",
       "      <th>D_68_2.0</th>\n",
       "      <th>D_68_3.0</th>\n",
       "      <th>D_68_4.0</th>\n",
       "      <th>D_68_5.0</th>\n",
       "      <th>D_68_6.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-18</td>\n",
       "      <td>0.940705</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.018859</td>\n",
       "      <td>1.008024</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.103329</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>0.929122</td>\n",
       "      <td>0.382790</td>\n",
       "      <td>0.025782</td>\n",
       "      <td>1.002647</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.089799</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.005830</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>0.797670</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.819583</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-06</td>\n",
       "      <td>0.594502</td>\n",
       "      <td>0.007465</td>\n",
       "      <td>0.012893</td>\n",
       "      <td>1.001205</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.157413</td>\n",
       "      <td>0.006188</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-22</td>\n",
       "      <td>0.861652</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.813898</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82970</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>0.574043</td>\n",
       "      <td>0.419203</td>\n",
       "      <td>1.117365</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.007703</td>\n",
       "      <td>0.177751</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>0.969195</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82971</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>0.217687</td>\n",
       "      <td>0.007107</td>\n",
       "      <td>0.721968</td>\n",
       "      <td>0.022152</td>\n",
       "      <td>0.507603</td>\n",
       "      <td>0.177678</td>\n",
       "      <td>0.005980</td>\n",
       "      <td>0.977406</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82972</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>0.205260</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.220643</td>\n",
       "      <td>0.049644</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.248024</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.215941</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82973</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>0.461739</td>\n",
       "      <td>0.180809</td>\n",
       "      <td>0.513882</td>\n",
       "      <td>0.023612</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.374782</td>\n",
       "      <td>0.003630</td>\n",
       "      <td>0.920515</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82974</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>0.481917</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.182513</td>\n",
       "      <td>0.192125</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>0.393390</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.239441</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82975 rows × 225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target        S_2       P_2      D_39       B_1       B_2       R_1  \\\n",
       "0           0 2017-09-18  0.940705  0.002183  0.018859  1.008024  0.004509   \n",
       "1           0 2017-03-01  0.929122  0.382790  0.025782  1.002647  0.005515   \n",
       "2           0 2017-11-11  0.797670  0.002817  0.003238  0.819583  0.008977   \n",
       "3           0 2017-08-06  0.594502  0.007465  0.012893  1.001205  0.009915   \n",
       "4           0 2017-09-22  0.861652  0.007474  0.006711  0.813898  0.000336   \n",
       "...       ...        ...       ...       ...       ...       ...       ...   \n",
       "82970       1 2017-10-23  0.574043  0.419203  1.117365  0.021132  0.007703   \n",
       "82971       1 2017-06-22  0.217687  0.007107  0.721968  0.022152  0.507603   \n",
       "82972       0 2018-01-24  0.205260  0.001096  0.220643  0.049644  0.009036   \n",
       "82973       1 2017-11-01  0.461739  0.180809  0.513882  0.023612  0.001811   \n",
       "82974       0 2017-03-29  0.481917  0.009878  0.182513  0.192125  0.005269   \n",
       "\n",
       "            S_3      D_41       B_3  ...  D_126_1.0  D_66_0.0  D_66_1.0  \\\n",
       "0      0.103329  0.006603  0.000783  ...          1         0         0   \n",
       "1      0.089799  0.001479  0.005830  ...          1         0         0   \n",
       "2           NaN  0.003330  0.003115  ...          1         0         0   \n",
       "3      0.157413  0.006188  0.004458  ...          1         0         0   \n",
       "4           NaN  0.004766  0.000812  ...          1         0         1   \n",
       "...         ...       ...       ...  ...        ...       ...       ...   \n",
       "82970  0.177751  0.008778  0.969195  ...          1         0         0   \n",
       "82971  0.177678  0.005980  0.977406  ...          0         0         0   \n",
       "82972  1.248024  0.007380  0.215941  ...          0         0         0   \n",
       "82973  0.374782  0.003630  0.920515  ...          1         0         0   \n",
       "82974  0.393390  0.004422  0.239441  ...          1         0         0   \n",
       "\n",
       "       D_68_0.0  D_68_1.0  D_68_2.0  D_68_3.0  D_68_4.0  D_68_5.0  D_68_6.0  \n",
       "0             0         0         0         0         0         0         1  \n",
       "1             0         0         0         0         0         0         1  \n",
       "2             0         0         0         0         0         0         1  \n",
       "3             0         0         0         1         0         0         0  \n",
       "4             0         0         0         0         0         0         1  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "82970         0         0         0         0         1         0         0  \n",
       "82971         0         0         0         1         0         0         0  \n",
       "82972         0         0         0         0         0         0         1  \n",
       "82973         0         0         0         0         0         1         0  \n",
       "82974         0         0         0         0         0         0         1  \n",
       "\n",
       "[82975 rows x 225 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71cf341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes=final_data.dtypes\n",
    "\n",
    "# dtypelist1 = list()\n",
    "\n",
    "# for col in dtypes.index:\n",
    "#     if dtypes[col] == 'object':\n",
    "#         dtypelist1.append(col)\n",
    "        \n",
    "# print(dtypelist1)\n",
    "\n",
    "# final_data['D_64'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d329ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.dtypes\n",
    "\n",
    "final_data.replace('', nm.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ff59e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(\"XGBoost_Input_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b474956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82975, 225)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83abd515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "value = final_data['D_42'][1]\n",
    "\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8fe213e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2017-09\n",
       "1        2017-03\n",
       "2        2017-11\n",
       "3        2017-08\n",
       "4        2017-09\n",
       "          ...   \n",
       "82970    2017-10\n",
       "82971    2017-06\n",
       "82972    2018-01\n",
       "82973    2017-11\n",
       "82974    2017-03\n",
       "Name: Year-Month, Length: 82975, dtype: period[M]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['Year-Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b4b1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test-train-test split\n",
    "\n",
    "train_start_date = '2017-05'\n",
    "train_end_date = '2018-01'\n",
    "\n",
    "# Define the start and end dates for the test 1 set\n",
    "test_start_date = '2017-03'\n",
    "test_end_date = '2017-04'\n",
    "\n",
    "# Define the start and end dates for the test 2 setYear-Month\n",
    "test1_start_date = '2018-02'\n",
    "test1_end_date = '2018-03'\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_final_data = final_data[(final_data['Year-Month'] >= train_start_date) & (final_data['Year-Month'] <= train_end_date)]\n",
    "test_final_data = final_data[(final_data['Year-Month'] >= test_start_date) & (final_data['Year-Month'] <= test_end_date)]\n",
    "test2_final_data = final_data[(final_data['Year-Month'] >= test1_start_date) & (final_data['Year-Month'] <= test1_end_date) ]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00752e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55828, 225)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final_data['Year-Month'].max()\n",
    "\n",
    "train_final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abce343c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        2017-03\n",
       "5        2017-04\n",
       "6        2017-03\n",
       "13       2017-04\n",
       "18       2017-04\n",
       "          ...   \n",
       "82932    2017-03\n",
       "82961    2017-04\n",
       "82966    2017-04\n",
       "82969    2017-04\n",
       "82974    2017-03\n",
       "Name: Year-Month, Length: 11082, dtype: period[M]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final_data['Year-Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2631e060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9        2018-02\n",
       "10       2018-02\n",
       "12       2018-02\n",
       "14       2018-03\n",
       "20       2018-03\n",
       "          ...   \n",
       "82938    2018-03\n",
       "82942    2018-03\n",
       "82945    2018-03\n",
       "82950    2018-02\n",
       "82953    2018-03\n",
       "Name: Year-Month, Length: 16065, dtype: period[M]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2_final_data['Year-Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2865cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25999498459554343\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_final_data[\"target\"])/(len(train_final_data[\"target\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "223230d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_data.shape\n",
    "\n",
    "train_final_data['target'].dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51150390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14515\n",
      "Length of train: 55828\n",
      "Default Rate of train: 0.25999498459554343\n",
      "shape:  (55828, 225)\n"
     ]
    }
   ],
   "source": [
    "lentrain=len(train_final_data)\n",
    "DR1 = sum(train_final_data['target'])/lentrain\n",
    "print(sum(train_final_data['target']))\n",
    "print(\"Length of train:\",lentrain)\n",
    "print(\"Default Rate of train:\",DR1)\n",
    "print(\"shape: \",train_final_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00380808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 11082\n",
      "Default Rate of train: 0.2338025627143115\n",
      "2591\n",
      "shape:  (11082, 225)\n"
     ]
    }
   ],
   "source": [
    "lentest=len(test_final_data)\n",
    "DR2 = sum(test_final_data['target'])/lentest\n",
    "\n",
    "print(\"Length of train:\",lentest)\n",
    "print(\"Default Rate of train:\",DR2)\n",
    "print(sum(test_final_data['target']))\n",
    "print(\"shape: \",test_final_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9797b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 16065\n",
      "Default Rate of train: 0.2823529411764706\n",
      "4536\n",
      "Shape:  (16065, 225)\n"
     ]
    }
   ],
   "source": [
    "lentest1=len(test2_final_data)\n",
    "DR3 = sum(test2_final_data['target'])/lentest1\n",
    "\n",
    "print(\"Length of train:\",lentest1)\n",
    "print(\"Default Rate of train:\",DR3)\n",
    "print(sum(test2_final_data['target']))\n",
    "print(\"Shape: \",test2_final_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "652328cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>...</th>\n",
       "      <th>D_126_1.0</th>\n",
       "      <th>D_66_0.0</th>\n",
       "      <th>D_66_1.0</th>\n",
       "      <th>D_68_0.0</th>\n",
       "      <th>D_68_1.0</th>\n",
       "      <th>D_68_2.0</th>\n",
       "      <th>D_68_3.0</th>\n",
       "      <th>D_68_4.0</th>\n",
       "      <th>D_68_5.0</th>\n",
       "      <th>D_68_6.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-18</td>\n",
       "      <td>0.940705</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.018859</td>\n",
       "      <td>1.008024</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.103329</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>0.797670</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.819583</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-06</td>\n",
       "      <td>0.594502</td>\n",
       "      <td>0.007465</td>\n",
       "      <td>0.012893</td>\n",
       "      <td>1.001205</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.157413</td>\n",
       "      <td>0.006188</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-22</td>\n",
       "      <td>0.861652</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.813898</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-02</td>\n",
       "      <td>1.009520</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>0.030036</td>\n",
       "      <td>0.222870</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.094090</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82968</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-03</td>\n",
       "      <td>0.976103</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.050928</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.129943</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.003718</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82970</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>0.574043</td>\n",
       "      <td>0.419203</td>\n",
       "      <td>1.117365</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.007703</td>\n",
       "      <td>0.177751</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>0.969195</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82971</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>0.217687</td>\n",
       "      <td>0.007107</td>\n",
       "      <td>0.721968</td>\n",
       "      <td>0.022152</td>\n",
       "      <td>0.507603</td>\n",
       "      <td>0.177678</td>\n",
       "      <td>0.005980</td>\n",
       "      <td>0.977406</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82972</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>0.205260</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.220643</td>\n",
       "      <td>0.049644</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.248024</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.215941</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82973</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>0.461739</td>\n",
       "      <td>0.180809</td>\n",
       "      <td>0.513882</td>\n",
       "      <td>0.023612</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.374782</td>\n",
       "      <td>0.003630</td>\n",
       "      <td>0.920515</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55828 rows × 225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target        S_2       P_2      D_39       B_1       B_2       R_1  \\\n",
       "0           0 2017-09-18  0.940705  0.002183  0.018859  1.008024  0.004509   \n",
       "2           0 2017-11-11  0.797670  0.002817  0.003238  0.819583  0.008977   \n",
       "3           0 2017-08-06  0.594502  0.007465  0.012893  1.001205  0.009915   \n",
       "4           0 2017-09-22  0.861652  0.007474  0.006711  0.813898  0.000336   \n",
       "7           0 2017-06-02  1.009520  0.004791  0.030036  0.222870  0.008314   \n",
       "...       ...        ...       ...       ...       ...       ...       ...   \n",
       "82968       0 2017-11-03  0.976103  0.002395  0.050928  0.009312  0.003564   \n",
       "82970       1 2017-10-23  0.574043  0.419203  1.117365  0.021132  0.007703   \n",
       "82971       1 2017-06-22  0.217687  0.007107  0.721968  0.022152  0.507603   \n",
       "82972       0 2018-01-24  0.205260  0.001096  0.220643  0.049644  0.009036   \n",
       "82973       1 2017-11-01  0.461739  0.180809  0.513882  0.023612  0.001811   \n",
       "\n",
       "            S_3      D_41       B_3  ...  D_126_1.0  D_66_0.0  D_66_1.0  \\\n",
       "0      0.103329  0.006603  0.000783  ...          1         0         0   \n",
       "2           NaN  0.003330  0.003115  ...          1         0         0   \n",
       "3      0.157413  0.006188  0.004458  ...          1         0         0   \n",
       "4           NaN  0.004766  0.000812  ...          1         0         1   \n",
       "7      0.094183  0.000760  0.094090  ...          1         0         0   \n",
       "...         ...       ...       ...  ...        ...       ...       ...   \n",
       "82968  0.129943  0.000011  0.003718  ...          1         0         0   \n",
       "82970  0.177751  0.008778  0.969195  ...          1         0         0   \n",
       "82971  0.177678  0.005980  0.977406  ...          0         0         0   \n",
       "82972  1.248024  0.007380  0.215941  ...          0         0         0   \n",
       "82973  0.374782  0.003630  0.920515  ...          1         0         0   \n",
       "\n",
       "       D_68_0.0  D_68_1.0  D_68_2.0  D_68_3.0  D_68_4.0  D_68_5.0  D_68_6.0  \n",
       "0             0         0         0         0         0         0         1  \n",
       "2             0         0         0         0         0         0         1  \n",
       "3             0         0         0         1         0         0         0  \n",
       "4             0         0         0         0         0         0         1  \n",
       "7             0         0         0         0         0         1         0  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "82968         0         0         0         0         0         0         1  \n",
       "82970         0         0         0         0         1         0         0  \n",
       "82971         0         0         0         1         0         0         0  \n",
       "82972         0         0         0         0         0         0         1  \n",
       "82973         0         0         0         0         0         1         0  \n",
       "\n",
       "[55828 rows x 225 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6cc9b0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.8580581122541058\n",
      "Accuracy on test data 1: 0.8633675692499222\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the input features and target variable from the training data\n",
    "X_train = train_final_data.drop(columns=['target','S_2', 'Year-Month'])\n",
    "y_train = train_final_data[['target']]\n",
    "\n",
    "# Extract the input features and target variable from the test data\n",
    "X_test = test_final_data.drop(columns=['target','S_2', 'Year-Month'])\n",
    "y_test = test_final_data[['target']]\n",
    "\n",
    "# Extract the input features and target variable from the test data 1\n",
    "X_test1 = test2_final_data.drop(columns=['target','S_2', 'Year-Month'])\n",
    "y_test1 = test2_final_data[['target']]\n",
    "\n",
    "# Define the XGBoost model\n",
    "model = xgb.XGBClassifier(random_state=52)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test data:\", accuracy)\n",
    "\n",
    "# Make predictions on the test data 1\n",
    "y_pred1 = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test data 1\n",
    "accuracy1 = accuracy_score(y_test1, y_pred1)\n",
    "print(\"Accuracy on test data 1:\", accuracy1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "012d5743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41', 'B_3', 'D_42',\n",
       "       'D_43',\n",
       "       ...\n",
       "       'D_126_1.0', 'D_66_0.0', 'D_66_1.0', 'D_68_0.0', 'D_68_1.0', 'D_68_2.0',\n",
       "       'D_68_3.0', 'D_68_4.0', 'D_68_5.0', 'D_68_6.0'],\n",
       "      dtype='object', length=222)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01860e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9        0.075384\n",
       "10       0.013244\n",
       "12       0.067508\n",
       "14       0.071628\n",
       "20       0.012949\n",
       "           ...   \n",
       "82938    0.057730\n",
       "82942    0.164878\n",
       "82945    0.759262\n",
       "82950    0.042705\n",
       "82953    0.148401\n",
       "Name: D_45, Length: 16065, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1['D_45']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9348f804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        0.239459\n",
       "5        0.317302\n",
       "6        0.006476\n",
       "13       0.176870\n",
       "18       0.076816\n",
       "           ...   \n",
       "82932    0.060681\n",
       "82961    0.004656\n",
       "82966    0.317783\n",
       "82969    0.317409\n",
       "82974    0.561051\n",
       "Name: D_45, Length: 11082, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['D_45']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "999efbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9        1\n",
       "10       1\n",
       "12       1\n",
       "14       0\n",
       "20       1\n",
       "        ..\n",
       "82938    1\n",
       "82942    0\n",
       "82945    0\n",
       "82950    0\n",
       "82953    0\n",
       "Name: target, Length: 16065, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f41d4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23103616 0.00518782 0.04477808 0.01116603 0.0079173  0.00975707\n",
      " 0.00603193 0.0114038  0.01924665 0.00639195 0.0047933  0.00772059\n",
      " 0.01410126 0.00535661 0.00375378 0.00623055 0.00515429 0.00463113\n",
      " 0.00623911 0.00668139 0.00681088 0.00351257 0.00685481 0.00512357\n",
      " 0.02779971 0.00501386 0.00420863 0.00454043 0.00292304 0.00321036\n",
      " 0.00344321 0.00367231 0.00259333 0.00294891 0.00252772 0.00502288\n",
      " 0.00338084 0.00358826 0.00279491 0.00493208 0.00258918 0.00219815\n",
      " 0.00274349 0.00367599 0.00308364 0.00258661 0.00249804 0.00955228\n",
      " 0.00250099 0.00346386 0.00548843 0.00355243 0.0032297  0.00295423\n",
      " 0.00317869 0.00474911 0.00339026 0.00326985 0.00394148 0.00304388\n",
      " 0.00258772 0.00325156 0.00305224 0.00300729 0.00290626 0.00296841\n",
      " 0.00411604 0.0111678  0.00258413 0.00312451 0.00335961 0.01518833\n",
      " 0.00285971 0.00287424 0.00196461 0.00294226 0.00241281 0.00248415\n",
      " 0.00280523 0.00475436 0.00278324 0.00242142 0.00284536 0.00282793\n",
      " 0.00256198 0.00274176 0.00253525 0.00276669 0.00219368 0.00260488\n",
      " 0.00381855 0.00255274 0.00233921 0.00224842 0.00240472 0.00228703\n",
      " 0.00298292 0.00236488 0.00437279 0.00261649 0.00257536 0.\n",
      " 0.00241293 0.0023279  0.00093876 0.         0.00256969 0.00247237\n",
      " 0.0027474  0.00273569 0.00294059 0.00283578 0.00496474 0.0024885\n",
      " 0.00242078 0.0024696  0.00280112 0.0025442  0.00217265 0.00235676\n",
      " 0.00301828 0.00228804 0.00241214 0.00238366 0.00378942 0.00392745\n",
      " 0.00265075 0.00321692 0.00218137 0.00303053 0.00187777 0.00309751\n",
      " 0.00359105 0.00272516 0.0027867  0.00371409 0.00599387 0.00619191\n",
      " 0.0018501  0.00295059 0.00294601 0.00251929 0.00165982 0.00503089\n",
      " 0.00296126 0.00247563 0.00288939 0.00359303 0.00234478 0.0027141\n",
      " 0.0028918  0.00305162 0.00277613 0.00418437 0.00232332 0.00278606\n",
      " 0.00380142 0.00286712 0.00273461 0.00183213 0.00285285 0.00317811\n",
      " 0.00648757 0.00311566 0.00277669 0.00358758 0.00209453 0.00262131\n",
      " 0.00390418 0.00298885 0.00237726 0.00298024 0.00269887 0.00294704\n",
      " 0.00312463 0.00289581 0.00241254 0.00204996 0.         0.00449482\n",
      " 0.         0.         0.         0.         0.00768155 0.00206718\n",
      " 0.00460979 0.         0.         0.         0.         0.00070931\n",
      " 0.         0.         0.00109802 0.         0.00331253 0.00107944\n",
      " 0.         0.         0.00109297 0.         0.         0.00139412\n",
      " 0.00101085 0.00280536 0.         0.         0.00293412 0.\n",
      " 0.         0.         0.00321292 0.         0.00703618 0.\n",
      " 0.         0.         0.         0.00267965 0.         0.        ]\n",
      "      feature  importance\n",
      "0         P_2    0.231036\n",
      "2         B_1    0.044778\n",
      "24        B_9    0.027800\n",
      "8        D_42    0.019247\n",
      "71       D_75    0.015188\n",
      "..        ...         ...\n",
      "183   D_64_-1    0.000000\n",
      "182   D_63_XZ    0.000000\n",
      "181   D_63_XM    0.000000\n",
      "180   D_63_XL    0.000000\n",
      "221  D_68_6.0    0.000000\n",
      "\n",
      "[222 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Feature_Importance\n",
    "importance = model.feature_importances_\n",
    "\n",
    "print(importance)\n",
    "\n",
    "feature_importance_df = pd.DataFrame(list(zip(X_train.columns, importance)), columns=['feature', 'importance'])\n",
    "\n",
    "# sort the dataframe in descending order of importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# print the dataframe\n",
    "print(feature_importance_df)\n",
    "\n",
    "feature_importance_df.to_csv(\"Feature_Importance_1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11fc4280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55828, 222)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44d38dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09066728 0.00495808 0.00327127 0.00364331 0.00732646 0.01146796\n",
      " 0.00428374 0.01246363 0.00827903 0.00558746 0.00647013 0.00556065\n",
      " 0.01243175 0.00482604 0.00351773 0.00456799 0.00441733 0.00538781\n",
      " 0.00508951 0.00428201 0.04895436 0.00743476 0.00495223 0.00505983\n",
      " 0.01404354 0.00722865 0.00837224 0.00624648 0.00470223 0.0037911\n",
      " 0.00440767 0.00637604 0.00332211 0.0037099  0.00310473 0.00430568\n",
      " 0.00417249 0.00437793 0.00620094 0.00549598 0.00453495 0.00335222\n",
      " 0.00403197 0.00388843 0.00380249 0.00389086 0.00370659 0.00929049\n",
      " 0.00451525 0.0031373  0.00414055 0.00353015 0.00398813 0.00468448\n",
      " 0.00641808 0.00376982 0.00447006 0.00422022 0.00316115 0.00378284\n",
      " 0.00361413 0.00388118 0.00520495 0.00376652 0.00409967 0.00339376\n",
      " 0.0042085  0.00511498 0.00373618 0.0034657  0.00346157 0.00322186\n",
      " 0.00535879 0.00383625 0.00357586 0.0057577  0.00442404 0.00364119\n",
      " 0.0034718  0.00383994 0.00355971 0.00375538 0.00403261 0.00343118\n",
      " 0.00342429 0.00378472 0.00335057 0.00356376 0.00424618 0.0035991\n",
      " 0.00395223 0.00468113 0.00325834 0.00371198 0.00332218 0.00326608\n",
      " 0.00410022 0.0030948  0.00446284 0.00407126 0.00361942 0.0035153\n",
      " 0.00384252 0.00364975 0.         0.         0.00295113 0.00411089\n",
      " 0.0033918  0.00362271 0.00312944 0.00333287 0.00333811 0.00320167\n",
      " 0.00389727 0.0037983  0.00424853 0.00490053 0.00370243 0.00392377\n",
      " 0.00316837 0.00269902 0.00386275 0.00341535 0.00396754 0.00452937\n",
      " 0.00340265 0.00392208 0.00370122 0.00334708 0.00377411 0.00366766\n",
      " 0.00424677 0.00345611 0.00324132 0.01607889 0.00974384 0.00335821\n",
      " 0.         0.00368961 0.0041624  0.00611687 0.00687802 0.0052344\n",
      " 0.00357841 0.00336448 0.0036246  0.00420397 0.00353151 0.00338694\n",
      " 0.00413478 0.00379662 0.00348572 0.00399867 0.00366557 0.00398209\n",
      " 0.00425529 0.00396535 0.00365284 0.00385172 0.00333813 0.00386629\n",
      " 0.00725174 0.00331263 0.00393109 0.00349893 0.00400274 0.00389803\n",
      " 0.0040334  0.00457585 0.00320111 0.00392191 0.00352121 0.00365669\n",
      " 0.00381034 0.00359145 0.00295358 0.         0.00859517 0.00586829\n",
      " 0.         0.         0.         0.         0.00428463 0.00236449\n",
      " 0.0039094  0.         0.         0.         0.         0.00447329\n",
      " 0.00206425 0.         0.         0.00495128 0.00078271 0.00409556\n",
      " 0.00428666 0.         0.         0.00130696 0.         0.00437418\n",
      " 0.00458109 0.0045146  0.         0.00446278 0.00293555 0.\n",
      " 0.         0.         0.00178598 0.         0.00710601 0.\n",
      " 0.         0.00387891 0.00225007 0.         0.00762724 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# set the hyperparameters\n",
    "params = {\n",
    "    'n_estimators': 300,\n",
    "    'learning_rate': 0.5,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.5,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'scale_pos_weight': 5\n",
    "}\n",
    "\n",
    "# create the XGBClassifier object with the hyperparameters\n",
    "model1 = xgb.XGBClassifier(**params)\n",
    "\n",
    "# train the model on the training set\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "# # make predictions on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# print(y_pred)\n",
    "\n",
    "importance = model1.feature_importances_\n",
    "\n",
    "print(importance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b745556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        feature  importance\n",
      "0           P_2    0.231036\n",
      "2           B_1    0.044778\n",
      "24          B_9    0.027800\n",
      "8          D_42    0.019247\n",
      "71         D_75    0.015188\n",
      "..          ...         ...\n",
      "210  D_126_-1.0    0.000000\n",
      "211   D_126_0.0    0.000000\n",
      "215    D_68_0.0    0.000000\n",
      "213    D_66_0.0    0.000000\n",
      "221    D_68_6.0    0.000000\n",
      "\n",
      "[222 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame(list(zip(X_train.columns, importance)), columns=['feature', 'importance'])\n",
    "\n",
    "# sort the dataframe in descending order of importance\n",
    "feature_importance = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# print the dataframe\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9485fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.to_csv(\"Feature_Importance2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aba1b8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D_62',\n",
       " 'D_75',\n",
       " 'B_2',\n",
       " 'B_6',\n",
       " 'S_3',\n",
       " 'D_112',\n",
       " 'B_4',\n",
       " 'D_51',\n",
       " 'B_23',\n",
       " 'D_64_O',\n",
       " 'D_50',\n",
       " 'P_2',\n",
       " 'D_41',\n",
       " 'D_47',\n",
       " 'R_3',\n",
       " 'D_43',\n",
       " 'R_1',\n",
       " 'D_42',\n",
       " 'R_26',\n",
       " 'D_45',\n",
       " 'B_9',\n",
       " 'B_3',\n",
       " 'D_66_1.0',\n",
       " 'R_27',\n",
       " 'D_61',\n",
       " 'B_7',\n",
       " 'D_132',\n",
       " 'D_46',\n",
       " 'B_1',\n",
       " 'D_49',\n",
       " 'B_5',\n",
       " 'D_39',\n",
       " 'S_7']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected_features = set(feature_importance[feature_importance['importance'] > 0.005]['feature']) | set(feature_importance_df[feature_importance_df['importance']> 0.005]['feature'])\n",
    "df_selected_features= list(df_selected_features)\n",
    "df_selected_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "379d6062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=52, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.5, 1.0],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.1],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 300],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1, 5, 10],\n",
       "                         &#x27;subsample&#x27;: [0.5, 0.8]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=52, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.5, 1.0],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.1],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 300],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1, 5, 10],\n",
       "                         &#x27;subsample&#x27;: [0.5, 0.8]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=52, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=52, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric='auc', feature_types=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=52, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 1.0],\n",
       "                         'learning_rate': [0.01, 0.1],\n",
       "                         'n_estimators': [50, 100, 300],\n",
       "                         'scale_pos_weight': [1, 5, 10],\n",
       "                         'subsample': [0.5, 0.8]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset X_train to include only selected features\n",
    "X_train_selected = X_train[df_selected_features]\n",
    "X_test_selected= X_test[df_selected_features]\n",
    "X_test1_selected= X_test1[df_selected_features]\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 300],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.5, 0.8],\n",
    "    'colsample_bytree': [0.5, 1.0],\n",
    "    'scale_pos_weight': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Create an XGBoost classifier object\n",
    "xgb_model = xgb.XGBClassifier(random_state=52,objective='binary:logistic', eval_metric='auc')\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, n_jobs=-1,scoring='roc_auc')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de1b4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= []\n",
    "test1=[]\n",
    "test2=[]\n",
    "t=[]\n",
    "l=[]\n",
    "s=[]\n",
    "c=[]\n",
    "w=[]\n",
    "scores =pd.DataFrame()\n",
    "\n",
    "# Create an XGBoost classifier object\n",
    "for trees in [50,100,300]:\n",
    "    for LR in [0.01, 0.1]:\n",
    "        for subsample in [0.5,0.8]:\n",
    "            for colsample in [0.5,1]:\n",
    "                for weight in [1,5,10]:\n",
    "                    model_xgb_test= xgb.XGBClassifier(n_estimators=trees, learning_rate=LR, subsample=subsample,colsample_bytree=colsample, min_child_weight=weight, random_state=42)\n",
    "                    model_xgb_test.fit(X_train_selected, y_train)\n",
    "                    t.append(trees)\n",
    "                    l.append(LR)\n",
    "                    s.append(subsample)\n",
    "                    c.append(colsample)\n",
    "                    w.append(weight)\n",
    "                    train.append(roc_auc_score(y_train, model_xgb_test.predict(X_train_selected)))\n",
    "                    test1.append(roc_auc_score(y_test, model_xgb_test.predict(X_test_selected)))\n",
    "                    test2.append(roc_auc_score(y_test1, model_xgb_test.predict(X_test1_selected)))\n",
    "scores['trees'] = t\n",
    "scores['Learning rate'] = l\n",
    "scores['subsample'] = s\n",
    "scores['percentage features'] = c\n",
    "scores['Weight of default'] = w\n",
    "scores['AUC train 1'] = train\n",
    "scores['AUC test 1'] = test1\n",
    "scores['AUC test 2'] = test2\n",
    "scores.to_csv('XGBoost_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9834f7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trees</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>subsample</th>\n",
       "      <th>percentage features</th>\n",
       "      <th>Weight of default</th>\n",
       "      <th>AUC train 1</th>\n",
       "      <th>AUC test 1</th>\n",
       "      <th>AUC test 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818701</td>\n",
       "      <td>0.770073</td>\n",
       "      <td>0.839866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.818595</td>\n",
       "      <td>0.769955</td>\n",
       "      <td>0.839465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.817142</td>\n",
       "      <td>0.769259</td>\n",
       "      <td>0.838850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.825692</td>\n",
       "      <td>0.781889</td>\n",
       "      <td>0.843652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.824956</td>\n",
       "      <td>0.781235</td>\n",
       "      <td>0.845612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.892636</td>\n",
       "      <td>0.789495</td>\n",
       "      <td>0.853878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.885302</td>\n",
       "      <td>0.786953</td>\n",
       "      <td>0.852141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.910152</td>\n",
       "      <td>0.782767</td>\n",
       "      <td>0.853260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.899252</td>\n",
       "      <td>0.785184</td>\n",
       "      <td>0.852519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.890147</td>\n",
       "      <td>0.787271</td>\n",
       "      <td>0.856082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    trees  Learning rate  subsample  percentage features  Weight of default  \\\n",
       "0      50           0.01        0.5                  0.5                  1   \n",
       "1      50           0.01        0.5                  0.5                  5   \n",
       "2      50           0.01        0.5                  0.5                 10   \n",
       "3      50           0.01        0.5                  1.0                  1   \n",
       "4      50           0.01        0.5                  1.0                  5   \n",
       "..    ...            ...        ...                  ...                ...   \n",
       "67    300           0.10        0.8                  0.5                  5   \n",
       "68    300           0.10        0.8                  0.5                 10   \n",
       "69    300           0.10        0.8                  1.0                  1   \n",
       "70    300           0.10        0.8                  1.0                  5   \n",
       "71    300           0.10        0.8                  1.0                 10   \n",
       "\n",
       "    AUC train 1  AUC test 1  AUC test 2  \n",
       "0      0.818701    0.770073    0.839866  \n",
       "1      0.818595    0.769955    0.839465  \n",
       "2      0.817142    0.769259    0.838850  \n",
       "3      0.825692    0.781889    0.843652  \n",
       "4      0.824956    0.781235    0.845612  \n",
       "..          ...         ...         ...  \n",
       "67     0.892636    0.789495    0.853878  \n",
       "68     0.885302    0.786953    0.852141  \n",
       "69     0.910152    0.782767    0.853260  \n",
       "70     0.899252    0.785184    0.852519  \n",
       "71     0.890147    0.787271    0.856082  \n",
       "\n",
       "[72 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56edc9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_colsample_bytree', 'param_learning_rate', 'param_n_estimators', 'param_scale_pos_weight', 'param_subsample', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9149cb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92791578139507\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)\n",
    "# print(grid_search.scorer_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db059dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  7.49458661,   8.94182501,   7.8883883 ,   9.49664121,\n",
       "          7.88234839,   9.05600314,  16.32900047,  20.41911206,\n",
       "         16.01450758,  18.02597241,  15.46994405,  18.96880903,\n",
       "         46.18669634,  53.95826077,  47.08442993,  58.32613139,\n",
       "         48.13202295,  55.34702492,   7.63000398,   8.97767792,\n",
       "          7.82967844,   9.47281814,   8.33939886,  11.19322076,\n",
       "         16.39268761,  21.87748065,  19.39115119,  19.3483182 ,\n",
       "         15.85428286,  18.87141404,  47.45344701,  56.85473938,\n",
       "         48.30602241,  56.57305942,  49.89750581,  57.98028026,\n",
       "         15.68401079,  18.30793982,  15.32874351,  18.71092811,\n",
       "         15.51798339,  18.77881708,  30.16452289,  36.62112212,\n",
       "         31.90263977,  41.79111519,  34.473279  ,  39.89571314,\n",
       "         94.61574817, 115.22451768, 101.14213557, 119.49618316,\n",
       "         96.32532945, 117.72878709,  15.38959618,  19.7990778 ,\n",
       "         16.69061322,  20.09553781,  15.77510238,  19.41861825,\n",
       "         31.81260257,  38.42645001,  31.44280376,  37.42157855,\n",
       "         32.37620945,  40.13705053,  91.42591262, 109.45040565,\n",
       "         95.61928349, 113.71786036,  94.94886322,  98.44103341]),\n",
       " 'std_fit_time': array([0.11881604, 0.20742652, 0.09712469, 0.12147744, 0.08746966,\n",
       "        0.09458265, 1.0582405 , 0.18652454, 0.27236097, 0.21649019,\n",
       "        0.4212101 , 0.2189429 , 0.22356886, 0.18221577, 0.51602085,\n",
       "        0.36076924, 0.93474623, 0.33270843, 0.14094048, 0.12074174,\n",
       "        0.23659016, 0.02976094, 0.53690562, 0.0660972 , 0.27364717,\n",
       "        1.57642427, 0.56441782, 0.94691415, 0.10741811, 0.30716444,\n",
       "        1.07685024, 0.52730361, 0.27968636, 0.75946716, 0.77134683,\n",
       "        0.2737709 , 0.41181577, 0.22784029, 0.07030924, 0.12513598,\n",
       "        0.06871431, 0.14997027, 0.11363265, 0.38155703, 0.50234505,\n",
       "        0.62983002, 0.66593895, 0.08663262, 0.23937187, 2.11907118,\n",
       "        0.78536095, 1.45661924, 1.19286399, 0.55708769, 0.04892449,\n",
       "        1.01934412, 0.26816312, 0.27313878, 0.11585896, 0.12113971,\n",
       "        0.33801944, 0.21253225, 0.30027298, 0.12061656, 1.10551911,\n",
       "        0.42754544, 0.7325965 , 1.97412345, 0.20412602, 0.71445464,\n",
       "        3.50828241, 1.99332759]),\n",
       " 'mean_score_time': array([0.05219164, 0.05815163, 0.05774255, 0.05081329, 0.04841166,\n",
       "        0.06538877, 0.10100336, 0.10732079, 0.07444329, 0.07182055,\n",
       "        0.08196101, 0.07984734, 0.19884043, 0.19949121, 0.19854236,\n",
       "        0.21066685, 0.19246311, 0.20271792, 0.04425988, 0.04505839,\n",
       "        0.04956307, 0.05060673, 0.0724956 , 0.05193753, 0.08947687,\n",
       "        0.10098019, 0.07689877, 0.0737402 , 0.07515321, 0.0790823 ,\n",
       "        0.20195107, 0.18195324, 0.18533249, 0.18827219, 0.18471341,\n",
       "        0.18233809, 0.05974627, 0.05391674, 0.05202131, 0.05493507,\n",
       "        0.05379772, 0.05057716, 0.09549837, 0.08543324, 0.09387589,\n",
       "        0.10782862, 0.09602041, 0.08146129, 0.22632737, 0.23231139,\n",
       "        0.22817807, 0.21648235, 0.21820531, 0.22509708, 0.05121369,\n",
       "        0.05748835, 0.05333838, 0.0518712 , 0.0480978 , 0.0562829 ,\n",
       "        0.08980665, 0.09033141, 0.08723769, 0.08813295, 0.09376936,\n",
       "        0.08028765, 0.20470786, 0.1861764 , 0.20214515, 0.21569028,\n",
       "        0.18486876, 0.16290569]),\n",
       " 'std_score_time': array([0.00513671, 0.00978169, 0.00712795, 0.01071086, 0.00292451,\n",
       "        0.02041597, 0.00690164, 0.04721364, 0.00789111, 0.00443762,\n",
       "        0.01157652, 0.00688012, 0.00515229, 0.00723721, 0.0058676 ,\n",
       "        0.0167332 , 0.01327455, 0.01262647, 0.00096938, 0.00139184,\n",
       "        0.00533573, 0.0058467 , 0.03482216, 0.0036294 , 0.01625901,\n",
       "        0.01503443, 0.0102166 , 0.00446364, 0.00426151, 0.00464056,\n",
       "        0.02300481, 0.0061846 , 0.01223818, 0.01834205, 0.01014579,\n",
       "        0.00800265, 0.00385118, 0.00595888, 0.00557824, 0.002946  ,\n",
       "        0.00355762, 0.00326174, 0.0158237 , 0.005249  , 0.01333378,\n",
       "        0.01574151, 0.01754785, 0.0031246 , 0.01176336, 0.02678859,\n",
       "        0.02390007, 0.01342867, 0.01357201, 0.02534278, 0.00308004,\n",
       "        0.00929255, 0.00193303, 0.00281508, 0.00506763, 0.00526498,\n",
       "        0.01058326, 0.00929258, 0.00457256, 0.01495743, 0.01786473,\n",
       "        0.00426151, 0.01674204, 0.00924656, 0.01251439, 0.01262893,\n",
       "        0.01551371, 0.07779679]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[50, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100, 100,\n",
       "                    300, 300, 300, 300, 300, 300, 50, 50, 50, 50, 50, 50,\n",
       "                    100, 100, 100, 100, 100, 100, 300, 300, 300, 300, 300,\n",
       "                    300, 50, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100,\n",
       "                    100, 300, 300, 300, 300, 300, 300, 50, 50, 50, 50, 50,\n",
       "                    50, 100, 100, 100, 100, 100, 100, 300, 300, 300, 300,\n",
       "                    300, 300],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_scale_pos_weight': masked_array(data=[1, 1, 5, 5, 10, 10, 1, 1, 5, 5, 10, 10, 1, 1, 5, 5, 10,\n",
       "                    10, 1, 1, 5, 5, 10, 10, 1, 1, 5, 5, 10, 10, 1, 1, 5, 5,\n",
       "                    10, 10, 1, 1, 5, 5, 10, 10, 1, 1, 5, 5, 10, 10, 1, 1,\n",
       "                    5, 5, 10, 10, 1, 1, 5, 5, 10, 10, 1, 1, 5, 5, 10, 10,\n",
       "                    1, 1, 5, 5, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_subsample': masked_array(data=[0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5,\n",
       "                    0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8,\n",
       "                    0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5,\n",
       "                    0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8,\n",
       "                    0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5,\n",
       "                    0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8,\n",
       "                    0.5, 0.8, 0.5, 0.8, 0.5, 0.8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8}],\n",
       " 'split0_test_score': array([0.92145433, 0.92121797, 0.91830297, 0.91853376, 0.9166267 ,\n",
       "        0.91667838, 0.92320109, 0.92304795, 0.92014147, 0.9199051 ,\n",
       "        0.91805585, 0.91862677, 0.92679221, 0.92685222, 0.92516347,\n",
       "        0.92506531, 0.92359617, 0.92392028, 0.92693162, 0.92699088,\n",
       "        0.92578485, 0.92637272, 0.92470077, 0.9255924 , 0.92784868,\n",
       "        0.92736722, 0.92651857, 0.92689885, 0.92470629, 0.92623767,\n",
       "        0.92531686, 0.92613095, 0.92299623, 0.92468351, 0.92156428,\n",
       "        0.92460447, 0.91877495, 0.91684757, 0.91583904, 0.91448634,\n",
       "        0.91380774, 0.91302664, 0.92120242, 0.91993018, 0.91789814,\n",
       "        0.91719075, 0.91550802, 0.91552686, 0.92598229, 0.92533385,\n",
       "        0.92424338, 0.92419909, 0.9225653 , 0.92265062, 0.92657985,\n",
       "        0.92676872, 0.92540224, 0.92567903, 0.92500807, 0.92442963,\n",
       "        0.92699521, 0.92725704, 0.92546792, 0.92661195, 0.92532872,\n",
       "        0.92528607, 0.92316803, 0.92491986, 0.92289554, 0.92454169,\n",
       "        0.92134748, 0.92297797]),\n",
       " 'split1_test_score': array([0.92009701, 0.9196226 , 0.9184271 , 0.91855898, 0.91704711,\n",
       "        0.91726435, 0.92128313, 0.92106004, 0.91981993, 0.92002318,\n",
       "        0.91867559, 0.91904332, 0.92461251, 0.92478281, 0.92400274,\n",
       "        0.9239529 , 0.92269599, 0.92295144, 0.92582717, 0.92552764,\n",
       "        0.9247994 , 0.92520782, 0.92351163, 0.92367492, 0.92580762,\n",
       "        0.92569606, 0.92486802, 0.92519842, 0.92394683, 0.92411098,\n",
       "        0.92321831, 0.92364353, 0.92231273, 0.92280701, 0.92036688,\n",
       "        0.92189162, 0.91717245, 0.91562762, 0.91637188, 0.91532355,\n",
       "        0.91465334, 0.91385701, 0.91916076, 0.91849159, 0.91813382,\n",
       "        0.91765956, 0.91700146, 0.91616653, 0.92417583, 0.92364666,\n",
       "        0.92337797, 0.92351659, 0.92224363, 0.92212832, 0.92485141,\n",
       "        0.92454652, 0.92380611, 0.92495363, 0.92428589, 0.92414308,\n",
       "        0.9254305 , 0.92520509, 0.92333948, 0.92512993, 0.92405904,\n",
       "        0.92426883, 0.92331548, 0.92354679, 0.92074744, 0.9237243 ,\n",
       "        0.92092838, 0.92251065]),\n",
       " 'split2_test_score': array([0.92609285, 0.92563261, 0.92434681, 0.92402077, 0.92159473,\n",
       "        0.92218025, 0.92729045, 0.92756835, 0.92585335, 0.92580958,\n",
       "        0.92349137, 0.92385076, 0.93119928, 0.93121187, 0.9301638 ,\n",
       "        0.93021614, 0.92863699, 0.92875232, 0.93101808, 0.93125892,\n",
       "        0.93032699, 0.93082409, 0.92957858, 0.92969985, 0.93141275,\n",
       "        0.93222844, 0.93108701, 0.93158361, 0.9301569 , 0.93073919,\n",
       "        0.92900416, 0.93025108, 0.92875953, 0.92910075, 0.92623849,\n",
       "        0.92849985, 0.92378665, 0.92222127, 0.9215081 , 0.92122808,\n",
       "        0.91913883, 0.91895242, 0.92616795, 0.92512728, 0.92352664,\n",
       "        0.92319419, 0.92119926, 0.92115705, 0.93066352, 0.93019159,\n",
       "        0.9294877 , 0.92910709, 0.92769016, 0.9275375 , 0.93066146,\n",
       "        0.93097031, 0.93049844, 0.92995286, 0.92935157, 0.92985706,\n",
       "        0.93102996, 0.93234586, 0.93038115, 0.93005917, 0.92970523,\n",
       "        0.9303247 , 0.92810592, 0.93086117, 0.92699223, 0.92824028,\n",
       "        0.92691619, 0.92821718]),\n",
       " 'split3_test_score': array([0.92158964, 0.92146047, 0.91921751, 0.9189971 , 0.91764127,\n",
       "        0.91727395, 0.92317224, 0.92296828, 0.92090112, 0.92081539,\n",
       "        0.91907425, 0.91905643, 0.92614217, 0.92591427, 0.92488532,\n",
       "        0.92476358, 0.92371405, 0.92378657, 0.92656696, 0.92655154,\n",
       "        0.92453201, 0.92580064, 0.92433224, 0.92514628, 0.92648845,\n",
       "        0.92669031, 0.92530461, 0.92621213, 0.92460643, 0.92589059,\n",
       "        0.92393265, 0.92504838, 0.92208531, 0.92447554, 0.92239618,\n",
       "        0.92336914, 0.91760871, 0.91713518, 0.91633581, 0.91505057,\n",
       "        0.91378748, 0.91363499, 0.9203369 , 0.91955794, 0.91812965,\n",
       "        0.91718579, 0.91606017, 0.9157057 , 0.9254161 , 0.92467815,\n",
       "        0.92417964, 0.92369041, 0.92245739, 0.92208377, 0.92519865,\n",
       "        0.92520488, 0.92546598, 0.92563649, 0.92330656, 0.92470593,\n",
       "        0.92569932, 0.92610704, 0.92561996, 0.92607644, 0.9241875 ,\n",
       "        0.92513148, 0.92331946, 0.92424791, 0.92296306, 0.92350746,\n",
       "        0.92047734, 0.9231407 ]),\n",
       " 'split4_test_score': array([0.92095671, 0.92127736, 0.91849207, 0.91853522, 0.91603931,\n",
       "        0.91614408, 0.92257208, 0.92267874, 0.92035368, 0.9203517 ,\n",
       "        0.91835696, 0.91837827, 0.92646911, 0.92654028, 0.92478705,\n",
       "        0.92495451, 0.92329526, 0.92339766, 0.92667493, 0.92715759,\n",
       "        0.9260238 , 0.92587114, 0.92364999, 0.92535967, 0.9269584 ,\n",
       "        0.92759687, 0.92619268, 0.92632266, 0.92396183, 0.92597936,\n",
       "        0.92468509, 0.92651314, 0.92323655, 0.92480158, 0.92132182,\n",
       "        0.92374753, 0.91887377, 0.91798039, 0.91640075, 0.91643533,\n",
       "        0.91417605, 0.91370303, 0.92121425, 0.92058975, 0.91839451,\n",
       "        0.91808447, 0.91612353, 0.91622793, 0.92622522, 0.92578625,\n",
       "        0.92445613, 0.92432632, 0.92237364, 0.922241  , 0.92645558,\n",
       "        0.92722747, 0.92504207, 0.92659536, 0.92387421, 0.92371184,\n",
       "        0.9267431 , 0.92805452, 0.92528504, 0.92692313, 0.92516854,\n",
       "        0.92473852, 0.92370859, 0.9261833 , 0.9218614 , 0.92411351,\n",
       "        0.92178389, 0.92278399]),\n",
       " 'mean_test_score': array([0.92203811, 0.9218422 , 0.91975729, 0.91972917, 0.91778983,\n",
       "        0.9179082 , 0.9235038 , 0.92346467, 0.92141391, 0.92138099,\n",
       "        0.9195308 , 0.91979111, 0.92704306, 0.92706029, 0.92580048,\n",
       "        0.92579049, 0.92438769, 0.92456165, 0.92740375, 0.92749731,\n",
       "        0.92629341, 0.92681528, 0.92515464, 0.92589462, 0.92770318,\n",
       "        0.92791578, 0.92679418, 0.92724313, 0.92547566, 0.92659156,\n",
       "        0.92523141, 0.92631742, 0.92387807, 0.92517368, 0.92237753,\n",
       "        0.92442252, 0.9192433 , 0.9179624 , 0.91729112, 0.91650477,\n",
       "        0.91511269, 0.91463482, 0.92161646, 0.92073935, 0.91921655,\n",
       "        0.91866295, 0.91717849, 0.91695681, 0.92649259, 0.9259273 ,\n",
       "        0.92514896, 0.9249679 , 0.92346602, 0.92332824, 0.92674939,\n",
       "        0.92694358, 0.92604297, 0.92656347, 0.92516526, 0.92536951,\n",
       "        0.92717962, 0.92779391, 0.92601871, 0.92696012, 0.92568981,\n",
       "        0.92594992, 0.9243235 , 0.92595181, 0.92309193, 0.92482545,\n",
       "        0.92229066, 0.9239261 ]),\n",
       " 'std_test_score': array([0.00209381, 0.0020074 , 0.00231691, 0.00215303, 0.00197323,\n",
       "        0.00217678, 0.00201692, 0.00217532, 0.00224747, 0.00223668,\n",
       "        0.00200895, 0.00204605, 0.00220842, 0.00219315, 0.00221544,\n",
       "        0.00224688, 0.0021537 , 0.00212233, 0.00184409, 0.00196447,\n",
       "        0.00209455, 0.00203819, 0.00225464, 0.00201715, 0.00197005,\n",
       "        0.00225524, 0.00222689, 0.00223827, 0.0023618 , 0.00220669,\n",
       "        0.00201389, 0.00220485, 0.00247718, 0.00209233, 0.00203597,\n",
       "        0.00221939, 0.00236467, 0.00225905, 0.00211858, 0.00244535,\n",
       "        0.00203751, 0.00217719, 0.00239654, 0.00229692, 0.00216076,\n",
       "        0.00229011, 0.00206649, 0.00211697, 0.00220249, 0.00225024,\n",
       "        0.00220003, 0.0020916 , 0.00211469, 0.00211412, 0.0020701 ,\n",
       "        0.0022403 , 0.00230651, 0.00177325, 0.0021654 , 0.00226777,\n",
       "        0.00201496, 0.00247483, 0.0023328 , 0.00166476, 0.00207085,\n",
       "        0.00221553, 0.00189969, 0.00260414, 0.00211085, 0.00174345,\n",
       "        0.00235312, 0.00215579]),\n",
       " 'rank_test_score': array([51, 52, 58, 59, 66, 65, 44, 46, 54, 55, 60, 57,  9,  8, 26, 27, 40,\n",
       "        38,  5,  4, 19, 12, 34, 25,  3,  1, 13,  6, 29, 15, 31, 18, 43, 32,\n",
       "        49, 39, 61, 64, 67, 70, 71, 72, 53, 56, 62, 63, 68, 69, 17, 24, 35,\n",
       "        36, 45, 47, 14, 11, 20, 16, 33, 30,  7,  2, 21, 10, 28, 23, 41, 22,\n",
       "        48, 37, 50, 42], dtype=int32)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5145c94-bff4-4867-a6e6-e365a8d89d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalization of data for Neural Networks\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc= StandardScaler()\n",
    "\n",
    "sc.fit(X_train_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "010061af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9220381083828098\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9218422025575534\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.91975729407038\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9197291667863208\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9177898251435618\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.917908201334049\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9235037997997692\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9234646687135755\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9214139100818137\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9213809907996048\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9195308026014539\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9197911092146762\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9270430567412655\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9270602911006052\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9258004753968743\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9257904879631422\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.924387691166926\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9245616527500398\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9274037516907911\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9274973117661348\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9262934120659292\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9268152819427755\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9251546395071767\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9258946238079409\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9277031794050746\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.92791578139507\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9267941792209697\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9272431341896331\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9254756585290759\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9265915576111828\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9252314115685817\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9263174155751882\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9238780721848616\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.92517367659003\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9223775303314354\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9244225202536043\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9192433047797707\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9179624039887999\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9172911173338477\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9165047729969441\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9151126871022539\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9146348186947384\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9216164561701049\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.920739348322296\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9192165501742806\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9186629520878888\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9171784869127286\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9169568111385422\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.926492592157139\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9259272981690426\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9251489648593665\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.924967897161259\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9234660246804556\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9233282406074291\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9267493888352796\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9269435797228389\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9260429678560594\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.926563472558955\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9251652589878916\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9253695078862997\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9271796196787528\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9277939106591901\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9260187064781711\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9269601229019535\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9256898058866089\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.925949920582329\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9243234956557218\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9259518059752183\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9230919336777399\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9248254466623814\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9222906581373493\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9239260993043322\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = grid_search.cv_results_\n",
    "for i in range(len(results['params'])):\n",
    "    print(\"Parameters\",results['params'][i])\n",
    "    print(\"AUC Score\",results['mean_test_score'][i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "286985dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.0...</td>\n",
       "      <td>0.922038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.0...</td>\n",
       "      <td>0.921842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.0...</td>\n",
       "      <td>0.919757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.0...</td>\n",
       "      <td>0.919729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.0...</td>\n",
       "      <td>0.917790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.925952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.923092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.924825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.922291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.923926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  auc_score\n",
       "0   {'colsample_bytree': 0.5, 'learning_rate': 0.0...   0.922038\n",
       "1   {'colsample_bytree': 0.5, 'learning_rate': 0.0...   0.921842\n",
       "2   {'colsample_bytree': 0.5, 'learning_rate': 0.0...   0.919757\n",
       "3   {'colsample_bytree': 0.5, 'learning_rate': 0.0...   0.919729\n",
       "4   {'colsample_bytree': 0.5, 'learning_rate': 0.0...   0.917790\n",
       "..                                                ...        ...\n",
       "67  {'colsample_bytree': 1.0, 'learning_rate': 0.1...   0.925952\n",
       "68  {'colsample_bytree': 1.0, 'learning_rate': 0.1...   0.923092\n",
       "69  {'colsample_bytree': 1.0, 'learning_rate': 0.1...   0.924825\n",
       "70  {'colsample_bytree': 1.0, 'learning_rate': 0.1...   0.922291\n",
       "71  {'colsample_bytree': 1.0, 'learning_rate': 0.1...   0.923926\n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'params': results['params'],\n",
    "    'auc_score': results['mean_test_score']\n",
    "})\n",
    "\n",
    "# Write the dataframe to a CSV file\n",
    "results_df.to_csv('grid_search_results_xgb.csv', index=False)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3fc98a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC: 0.92791578139507\n",
      "Best parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_auc = grid_search.best_score_\n",
    "print(\"Best AUC:\", best_auc)\n",
    "print(\"Best parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3dade29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on test data: 0.917774994669362\n",
      "AUC score on test 2 data: 0.9402545367918919\n"
     ]
    }
   ],
   "source": [
    "#running xgb on hyper parameters\n",
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'scale_pos_weight': 1\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "xgb_best_model = xgb.XGBClassifier(**params)\n",
    "xgb_best_model.fit(X_train_selected, y_train)\n",
    "# Evaluate the model\n",
    "auc_score = roc_auc_score(y_test, xgb_best_model.predict_proba(X_test_selected)[:, 1])\n",
    "print(\"AUC score on test data:\", auc_score)\n",
    "\n",
    "auc_score1 = roc_auc_score(y_test1, xgb_best_model.predict_proba(X_test1_selected)[:, 1])\n",
    "print(\"AUC score on test 2 data:\", auc_score1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4749457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "      <th>Bad Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score Bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0.0, 0.00337]</th>\n",
       "      <td>1</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.000179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.00337, 0.00601]</th>\n",
       "      <td>4</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.000716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.00601, 0.0112]</th>\n",
       "      <td>13</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.002328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0112, 0.0243]</th>\n",
       "      <td>41</td>\n",
       "      <td>5582</td>\n",
       "      <td>0.007345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0243, 0.0619]</th>\n",
       "      <td>123</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.022031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0619, 0.18]</th>\n",
       "      <td>462</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.082751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.18, 0.4]</th>\n",
       "      <td>1407</td>\n",
       "      <td>5582</td>\n",
       "      <td>0.252060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.4, 0.631]</th>\n",
       "      <td>2838</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.508329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.631, 0.814]</th>\n",
       "      <td>4325</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.774673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.814, 1.0]</th>\n",
       "      <td>5301</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.949490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sum  count  Bad Rate\n",
       "Score Bins                               \n",
       "(0.0, 0.00337]         1   5583  0.000179\n",
       "(0.00337, 0.00601]     4   5583  0.000716\n",
       "(0.00601, 0.0112]     13   5583  0.002328\n",
       "(0.0112, 0.0243]      41   5582  0.007345\n",
       "(0.0243, 0.0619]     123   5583  0.022031\n",
       "(0.0619, 0.18]       462   5583  0.082751\n",
       "(0.18, 0.4]         1407   5582  0.252060\n",
       "(0.4, 0.631]        2838   5583  0.508329\n",
       "(0.631, 0.814]      4325   5583  0.774673\n",
       "(0.814, 1.0]        5301   5583  0.949490"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rank ordering\n",
    "\n",
    "perf_train_data = pd.DataFrame({\"Actual\": y_train['target'], \"Prediction\": xgb_best_model.predict_proba(X_train_selected)[:,1]})\n",
    "quantiles = list(set(perf_train_data.Prediction.quantile([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])))\n",
    "quantiles.sort()\n",
    "quantiles.insert(0,0)\n",
    "quantiles.insert(len(quantiles),1)\n",
    "perf_train_data[\"Score Bins\"] = pd.cut(perf_train_data[\"Prediction\"], quantiles)\n",
    "stat = perf_train_data.groupby(\"Score Bins\")[\"Actual\"].agg([\"sum\", \"count\"])\n",
    "stat[\"Bad Rate\"] = stat[\"sum\"] / stat[\"count\"]\n",
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5f502fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = stat.plot(kind='bar', y='Bad Rate', color='blue', alpha=0.7, legend=False)\n",
    "ax.set_xlabel(\"Score Bins\")\n",
    "ax.set_ylabel(\"Bad Rate\")\n",
    "ax.set_title(\"Bad Rate by Score Bins\")\n",
    "\n",
    "# display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1adc9374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/qtwpdq7n0g51j49ll8v0y0fm0000gn/T/ipykernel_97404/313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n",
      "/var/folders/zq/qtwpdq7n0g51j49ll8v0y0fm0000gn/T/ipykernel_97404/313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n",
      "/var/folders/zq/qtwpdq7n0g51j49ll8v0y0fm0000gn/T/ipykernel_97404/313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n",
      "/var/folders/zq/qtwpdq7n0g51j49ll8v0y0fm0000gn/T/ipykernel_97404/313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n",
      "/var/folders/zq/qtwpdq7n0g51j49ll8v0y0fm0000gn/T/ipykernel_97404/313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n",
      "/var/folders/zq/qtwpdq7n0g51j49ll8v0y0fm0000gn/T/ipykernel_97404/313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Minimum value</th>\n",
       "      <th>Maximum value</th>\n",
       "      <th>1 percentile</th>\n",
       "      <th>5 percentile</th>\n",
       "      <th>99 percentile</th>\n",
       "      <th>95 percentile</th>\n",
       "      <th>Median value</th>\n",
       "      <th>Mean value</th>\n",
       "      <th>Missing Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_2</td>\n",
       "      <td>-3.540211e-01</td>\n",
       "      <td>1.009994</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.218108</td>\n",
       "      <td>1.005452</td>\n",
       "      <td>0.973860</td>\n",
       "      <td>0.680646</td>\n",
       "      <td>0.648725</td>\n",
       "      <td>1.249774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D_45</td>\n",
       "      <td>5.261666e-06</td>\n",
       "      <td>1.567155</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.008279</td>\n",
       "      <td>0.993179</td>\n",
       "      <td>0.755202</td>\n",
       "      <td>0.153723</td>\n",
       "      <td>0.237450</td>\n",
       "      <td>0.075926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S_3</td>\n",
       "      <td>-3.380120e-01</td>\n",
       "      <td>3.000815</td>\n",
       "      <td>0.006532</td>\n",
       "      <td>0.063307</td>\n",
       "      <td>1.017716</td>\n",
       "      <td>0.618167</td>\n",
       "      <td>0.166007</td>\n",
       "      <td>0.230743</td>\n",
       "      <td>18.336848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B_9</td>\n",
       "      <td>7.369665e-09</td>\n",
       "      <td>14.255142</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.980806</td>\n",
       "      <td>0.650851</td>\n",
       "      <td>0.028445</td>\n",
       "      <td>0.191543</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D_42</td>\n",
       "      <td>-2.278093e-04</td>\n",
       "      <td>4.186066</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>0.007286</td>\n",
       "      <td>1.027596</td>\n",
       "      <td>0.579909</td>\n",
       "      <td>0.117514</td>\n",
       "      <td>0.185351</td>\n",
       "      <td>81.307623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D_50</td>\n",
       "      <td>-7.652144e-01</td>\n",
       "      <td>20.166578</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.025805</td>\n",
       "      <td>0.991982</td>\n",
       "      <td>0.449143</td>\n",
       "      <td>0.108546</td>\n",
       "      <td>0.168925</td>\n",
       "      <td>57.607713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature  Minimum value  Maximum value  1 percentile  5 percentile  \\\n",
       "0     P_2  -3.540211e-01       1.009994      0.001205      0.218108   \n",
       "1    D_45   5.261666e-06       1.567155      0.002799      0.008279   \n",
       "2     S_3  -3.380120e-01       3.000815      0.006532      0.063307   \n",
       "3     B_9   7.369665e-09      14.255142      0.000244      0.001204   \n",
       "4    D_42  -2.278093e-04       4.186066      0.002958      0.007286   \n",
       "5    D_50  -7.652144e-01      20.166578      0.002235      0.025805   \n",
       "\n",
       "   99 percentile  95 percentile  Median value  Mean value  Missing Value  \n",
       "0       1.005452       0.973860      0.680646    0.648725       1.249774  \n",
       "1       0.993179       0.755202      0.153723    0.237450       0.075926  \n",
       "2       1.017716       0.618167      0.166007    0.230743      18.336848  \n",
       "3       0.980806       0.650851      0.028445    0.191543       0.000000  \n",
       "4       1.027596       0.579909      0.117514    0.185351      81.307623  \n",
       "5       0.991982       0.449143      0.108546    0.168925      57.607713  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get summary stats\n",
    "\n",
    "def get_summary_statistics(df, column_names):\n",
    "    summary_stats = pd.DataFrame(columns=[\"Feature\", \"Minimum value\", \"Maximum value\", \"1 percentile\", \"5 percentile\", \"99 percentile\", \"95 percentile\", \"Median value\", \"Mean value\", \"Missing Value\"])\n",
    "    for column_name in column_names:\n",
    "        summary_stats = summary_stats.append({\n",
    "            \"Feature\": column_name,\n",
    "            \"Minimum value\": df[column_name].min(),\n",
    "            \"Maximum value\": df[column_name].max(),\n",
    "            \"1 percentile\": df[column_name].quantile(0.01),\n",
    "            \"5 percentile\": df[column_name].quantile(0.05),\n",
    "            \"99 percentile\": df[column_name].quantile(0.99),\n",
    "            \"95 percentile\": df[column_name].quantile(0.95),\n",
    "            \"Median value\": df[column_name].median(),\n",
    "            \"Mean value\": df[column_name].mean(),\n",
    "            \"Missing Value\": df[column_name].isnull().sum() / len(df[column_name]) * 100\n",
    "        }, ignore_index=True)\n",
    "    return summary_stats\n",
    "\n",
    "column_name_input = ['P_2','D_45','S_3','B_9','D_42','D_50']\n",
    "get_summary_statistics(final_data,column_name_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3f00ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9        0.075384\n",
      "10       0.013244\n",
      "12       0.067508\n",
      "14       0.071628\n",
      "20       0.012949\n",
      "           ...   \n",
      "82938    0.057730\n",
      "82942    0.164878\n",
      "82945    0.759262\n",
      "82950    0.042705\n",
      "82953    0.148401\n",
      "Name: D_45, Length: 16065, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_test1_selected['D_45'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2cb8e861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        0.239459\n",
      "5        0.317302\n",
      "6        0.006476\n",
      "13       0.176870\n",
      "18       0.076816\n",
      "           ...   \n",
      "82932    0.060681\n",
      "82961    0.004656\n",
      "82966    0.317783\n",
      "82969    0.317409\n",
      "82974    0.561051\n",
      "Name: D_45, Length: 11082, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_test_selected['D_45'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe2f12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = sc.transform(X_train_selected)\n",
    "X_test_normalized = sc.transform(X_test_selected)\n",
    "X_test1_normalized = sc.transform(X_test1_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eff8deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pandas DF\n",
    "X_train_normalized = pd.DataFrame(X_train_normalized,columns=X_train_selected.columns)\n",
    "X_test_normalized = pd.DataFrame(X_test_normalized,columns=X_test_selected.columns)\n",
    "X_test1_normalized = pd.DataFrame(X_test1_normalized,columns=X_test1_selected.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee2e31bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>50%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D_62</th>\n",
       "      <td>48189.0</td>\n",
       "      <td>-1.074167e-16</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>-0.845450</td>\n",
       "      <td>-0.820717</td>\n",
       "      <td>-0.430038</td>\n",
       "      <td>3.638394</td>\n",
       "      <td>13.777668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_75</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-8.896421e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.765725</td>\n",
       "      <td>-0.764539</td>\n",
       "      <td>-0.430422</td>\n",
       "      <td>3.754376</td>\n",
       "      <td>10.999495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_2</th>\n",
       "      <td>55779.0</td>\n",
       "      <td>7.477520e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-1.566445</td>\n",
       "      <td>-1.558451</td>\n",
       "      <td>0.477259</td>\n",
       "      <td>0.967569</td>\n",
       "      <td>0.968321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_6</th>\n",
       "      <td>55818.0</td>\n",
       "      <td>-6.110225e-18</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.223335</td>\n",
       "      <td>-0.213237</td>\n",
       "      <td>-0.093159</td>\n",
       "      <td>1.285272</td>\n",
       "      <td>112.535420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_3</th>\n",
       "      <td>45622.0</td>\n",
       "      <td>1.915671e-16</td>\n",
       "      <td>1.000011</td>\n",
       "      <td>-2.881064</td>\n",
       "      <td>-1.139953</td>\n",
       "      <td>-0.332569</td>\n",
       "      <td>3.997569</td>\n",
       "      <td>11.829200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_112</th>\n",
       "      <td>55766.0</td>\n",
       "      <td>-3.449125e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-2.342649</td>\n",
       "      <td>-2.337388</td>\n",
       "      <td>0.428788</td>\n",
       "      <td>0.444942</td>\n",
       "      <td>0.445272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_4</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-3.665478e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.773471</td>\n",
       "      <td>-0.770369</td>\n",
       "      <td>-0.402848</td>\n",
       "      <td>3.761549</td>\n",
       "      <td>12.494887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_51</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>1.360554e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.591783</td>\n",
       "      <td>-0.591157</td>\n",
       "      <td>-0.561613</td>\n",
       "      <td>3.631422</td>\n",
       "      <td>9.231972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_23</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-4.352755e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.748130</td>\n",
       "      <td>-0.742303</td>\n",
       "      <td>-0.483066</td>\n",
       "      <td>3.703861</td>\n",
       "      <td>5.449965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_64_O</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>6.770952e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-1.032779</td>\n",
       "      <td>-1.032779</td>\n",
       "      <td>0.968262</td>\n",
       "      <td>0.968262</td>\n",
       "      <td>0.968262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_50</th>\n",
       "      <td>23940.0</td>\n",
       "      <td>-3.769379e-17</td>\n",
       "      <td>1.000021</td>\n",
       "      <td>-2.406607</td>\n",
       "      <td>-0.454422</td>\n",
       "      <td>-0.164277</td>\n",
       "      <td>2.245851</td>\n",
       "      <td>52.433440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_2</th>\n",
       "      <td>55155.0</td>\n",
       "      <td>1.071837e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-4.149408</td>\n",
       "      <td>-2.634398</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>1.456492</td>\n",
       "      <td>1.474887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_41</th>\n",
       "      <td>55779.0</td>\n",
       "      <td>-6.751423e-18</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.287877</td>\n",
       "      <td>-0.287232</td>\n",
       "      <td>-0.257311</td>\n",
       "      <td>4.442563</td>\n",
       "      <td>35.561009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_47</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-1.061461e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-1.807406</td>\n",
       "      <td>-1.714800</td>\n",
       "      <td>-0.101906</td>\n",
       "      <td>2.539341</td>\n",
       "      <td>5.265901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_3</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>1.091371e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.578155</td>\n",
       "      <td>-0.577217</td>\n",
       "      <td>-0.534926</td>\n",
       "      <td>3.875620</td>\n",
       "      <td>30.830262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_43</th>\n",
       "      <td>38392.0</td>\n",
       "      <td>1.209470e-16</td>\n",
       "      <td>1.000013</td>\n",
       "      <td>-0.708973</td>\n",
       "      <td>-0.697607</td>\n",
       "      <td>-0.304335</td>\n",
       "      <td>3.895852</td>\n",
       "      <td>28.859419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_1</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>5.396398e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.348944</td>\n",
       "      <td>-0.348424</td>\n",
       "      <td>-0.322585</td>\n",
       "      <td>4.223893</td>\n",
       "      <td>10.995303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_42</th>\n",
       "      <td>9854.0</td>\n",
       "      <td>-1.528669e-16</td>\n",
       "      <td>1.000051</td>\n",
       "      <td>-0.765331</td>\n",
       "      <td>-0.753075</td>\n",
       "      <td>-0.261952</td>\n",
       "      <td>3.296312</td>\n",
       "      <td>16.431129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_26</th>\n",
       "      <td>6402.0</td>\n",
       "      <td>3.662591e-17</td>\n",
       "      <td>1.000078</td>\n",
       "      <td>-0.341714</td>\n",
       "      <td>-0.340933</td>\n",
       "      <td>-0.191259</td>\n",
       "      <td>3.439473</td>\n",
       "      <td>23.646797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_45</th>\n",
       "      <td>55779.0</td>\n",
       "      <td>-7.311919e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-1.001863</td>\n",
       "      <td>-0.989321</td>\n",
       "      <td>-0.323737</td>\n",
       "      <td>3.114469</td>\n",
       "      <td>5.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_9</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>5.485490e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.665995</td>\n",
       "      <td>-0.665134</td>\n",
       "      <td>-0.567922</td>\n",
       "      <td>2.742730</td>\n",
       "      <td>49.273844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_3</th>\n",
       "      <td>55779.0</td>\n",
       "      <td>-2.783370e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.551971</td>\n",
       "      <td>-0.550884</td>\n",
       "      <td>-0.510392</td>\n",
       "      <td>3.763759</td>\n",
       "      <td>5.681250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_66_1.0</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>2.278196e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>2.863369</td>\n",
       "      <td>2.863369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_27</th>\n",
       "      <td>54269.0</td>\n",
       "      <td>-4.816251e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-2.845383</td>\n",
       "      <td>-2.776491</td>\n",
       "      <td>0.363459</td>\n",
       "      <td>0.380876</td>\n",
       "      <td>0.381245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_61</th>\n",
       "      <td>49899.0</td>\n",
       "      <td>2.053353e-16</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>-1.304862</td>\n",
       "      <td>-1.275128</td>\n",
       "      <td>-0.154373</td>\n",
       "      <td>1.687847</td>\n",
       "      <td>9.921027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_7</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>4.072753e-18</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-1.123802</td>\n",
       "      <td>-0.798153</td>\n",
       "      <td>-0.472853</td>\n",
       "      <td>3.579970</td>\n",
       "      <td>4.568177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_132</th>\n",
       "      <td>5484.0</td>\n",
       "      <td>-1.021956e-16</td>\n",
       "      <td>1.000091</td>\n",
       "      <td>-0.896447</td>\n",
       "      <td>-0.828160</td>\n",
       "      <td>-0.185995</td>\n",
       "      <td>2.977399</td>\n",
       "      <td>18.877260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_46</th>\n",
       "      <td>42258.0</td>\n",
       "      <td>6.002739e-16</td>\n",
       "      <td>1.000012</td>\n",
       "      <td>-18.636096</td>\n",
       "      <td>-2.749854</td>\n",
       "      <td>-0.091835</td>\n",
       "      <td>3.120153</td>\n",
       "      <td>22.237831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_1</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>2.850927e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-4.152585</td>\n",
       "      <td>-0.583203</td>\n",
       "      <td>-0.432805</td>\n",
       "      <td>4.198745</td>\n",
       "      <td>5.627720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_49</th>\n",
       "      <td>5533.0</td>\n",
       "      <td>6.677792e-17</td>\n",
       "      <td>1.000090</td>\n",
       "      <td>-0.748051</td>\n",
       "      <td>-0.737183</td>\n",
       "      <td>-0.242332</td>\n",
       "      <td>3.012835</td>\n",
       "      <td>27.538811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_5</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>3.970935e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.255189</td>\n",
       "      <td>-0.254024</td>\n",
       "      <td>-0.209023</td>\n",
       "      <td>2.918403</td>\n",
       "      <td>93.608871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_39</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>2.345015e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.586205</td>\n",
       "      <td>-0.585441</td>\n",
       "      <td>-0.549304</td>\n",
       "      <td>3.154475</td>\n",
       "      <td>17.506654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_7</th>\n",
       "      <td>45622.0</td>\n",
       "      <td>-3.021465e-17</td>\n",
       "      <td>1.000011</td>\n",
       "      <td>-2.310479</td>\n",
       "      <td>-1.074423</td>\n",
       "      <td>-0.390278</td>\n",
       "      <td>3.793688</td>\n",
       "      <td>11.828617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count          mean       std        min        1%       50%  \\\n",
       "D_62      48189.0 -1.074167e-16  1.000010  -0.845450 -0.820717 -0.430038   \n",
       "D_75      55828.0 -8.896421e-17  1.000009  -0.765725 -0.764539 -0.430422   \n",
       "B_2       55779.0  7.477520e-17  1.000009  -1.566445 -1.558451  0.477259   \n",
       "B_6       55818.0 -6.110225e-18  1.000009  -0.223335 -0.213237 -0.093159   \n",
       "S_3       45622.0  1.915671e-16  1.000011  -2.881064 -1.139953 -0.332569   \n",
       "D_112     55766.0 -3.449125e-16  1.000009  -2.342649 -2.337388  0.428788   \n",
       "B_4       55828.0 -3.665478e-17  1.000009  -0.773471 -0.770369 -0.402848   \n",
       "D_51      55828.0  1.360554e-16  1.000009  -0.591783 -0.591157 -0.561613   \n",
       "B_23      55828.0 -4.352755e-17  1.000009  -0.748130 -0.742303 -0.483066   \n",
       "D_64_O    55828.0  6.770952e-17  1.000009  -1.032779 -1.032779  0.968262   \n",
       "D_50      23940.0 -3.769379e-17  1.000021  -2.406607 -0.454422 -0.164277   \n",
       "P_2       55155.0  1.071837e-16  1.000009  -4.149408 -2.634398  0.129944   \n",
       "D_41      55779.0 -6.751423e-18  1.000009  -0.287877 -0.287232 -0.257311   \n",
       "D_47      55828.0 -1.061461e-16  1.000009  -1.807406 -1.714800 -0.101906   \n",
       "R_3       55828.0  1.091371e-16  1.000009  -0.578155 -0.577217 -0.534926   \n",
       "D_43      38392.0  1.209470e-16  1.000013  -0.708973 -0.697607 -0.304335   \n",
       "R_1       55828.0  5.396398e-17  1.000009  -0.348944 -0.348424 -0.322585   \n",
       "D_42       9854.0 -1.528669e-16  1.000051  -0.765331 -0.753075 -0.261952   \n",
       "R_26       6402.0  3.662591e-17  1.000078  -0.341714 -0.340933 -0.191259   \n",
       "D_45      55779.0 -7.311919e-17  1.000009  -1.001863 -0.989321 -0.323737   \n",
       "B_9       55828.0  5.485490e-17  1.000009  -0.665995 -0.665134 -0.567922   \n",
       "B_3       55779.0 -2.783370e-17  1.000009  -0.551971 -0.550884 -0.510392   \n",
       "D_66_1.0  55828.0  2.278196e-17  1.000009  -0.349239 -0.349239 -0.349239   \n",
       "R_27      54269.0 -4.816251e-16  1.000009  -2.845383 -2.776491  0.363459   \n",
       "D_61      49899.0  2.053353e-16  1.000010  -1.304862 -1.275128 -0.154373   \n",
       "B_7       55828.0  4.072753e-18  1.000009  -1.123802 -0.798153 -0.472853   \n",
       "D_132      5484.0 -1.021956e-16  1.000091  -0.896447 -0.828160 -0.185995   \n",
       "D_46      42258.0  6.002739e-16  1.000012 -18.636096 -2.749854 -0.091835   \n",
       "B_1       55828.0  2.850927e-17  1.000009  -4.152585 -0.583203 -0.432805   \n",
       "D_49       5533.0  6.677792e-17  1.000090  -0.748051 -0.737183 -0.242332   \n",
       "B_5       55828.0  3.970935e-17  1.000009  -0.255189 -0.254024 -0.209023   \n",
       "D_39      55828.0  2.345015e-17  1.000009  -0.586205 -0.585441 -0.549304   \n",
       "S_7       45622.0 -3.021465e-17  1.000011  -2.310479 -1.074423 -0.390278   \n",
       "\n",
       "               99%         max  \n",
       "D_62      3.638394   13.777668  \n",
       "D_75      3.754376   10.999495  \n",
       "B_2       0.967569    0.968321  \n",
       "B_6       1.285272  112.535420  \n",
       "S_3       3.997569   11.829200  \n",
       "D_112     0.444942    0.445272  \n",
       "B_4       3.761549   12.494887  \n",
       "D_51      3.631422    9.231972  \n",
       "B_23      3.703861    5.449965  \n",
       "D_64_O    0.968262    0.968262  \n",
       "D_50      2.245851   52.433440  \n",
       "P_2       1.456492    1.474887  \n",
       "D_41      4.442563   35.561009  \n",
       "D_47      2.539341    5.265901  \n",
       "R_3       3.875620   30.830262  \n",
       "D_43      3.895852   28.859419  \n",
       "R_1       4.223893   10.995303  \n",
       "D_42      3.296312   16.431129  \n",
       "R_26      3.439473   23.646797  \n",
       "D_45      3.114469    5.486200  \n",
       "B_9       2.742730   49.273844  \n",
       "B_3       3.763759    5.681250  \n",
       "D_66_1.0  2.863369    2.863369  \n",
       "R_27      0.380876    0.381245  \n",
       "D_61      1.687847    9.921027  \n",
       "B_7       3.579970    4.568177  \n",
       "D_132     2.977399   18.877260  \n",
       "D_46      3.120153   22.237831  \n",
       "B_1       4.198745    5.627720  \n",
       "D_49      3.012835   27.538811  \n",
       "B_5       2.918403   93.608871  \n",
       "D_39      3.154475   17.506654  \n",
       "S_7       3.793688   11.828617  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Outlier Treatment\n",
    "X_train_normalized.describe(percentiles=[0.01,0.99]).transpose()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0997f7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>50%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D_62</th>\n",
       "      <td>9467.0</td>\n",
       "      <td>0.024106</td>\n",
       "      <td>1.025060</td>\n",
       "      <td>-0.844205</td>\n",
       "      <td>-0.816770</td>\n",
       "      <td>-0.378554</td>\n",
       "      <td>3.580413</td>\n",
       "      <td>23.525745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_75</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.040425</td>\n",
       "      <td>0.976330</td>\n",
       "      <td>-0.765714</td>\n",
       "      <td>-0.764696</td>\n",
       "      <td>-0.436095</td>\n",
       "      <td>3.476083</td>\n",
       "      <td>8.875216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_2</th>\n",
       "      <td>11078.0</td>\n",
       "      <td>0.056946</td>\n",
       "      <td>0.977543</td>\n",
       "      <td>-1.566291</td>\n",
       "      <td>-1.556970</td>\n",
       "      <td>0.478706</td>\n",
       "      <td>0.967644</td>\n",
       "      <td>0.968316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_6</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.003627</td>\n",
       "      <td>0.727327</td>\n",
       "      <td>-0.219524</td>\n",
       "      <td>-0.212509</td>\n",
       "      <td>-0.081762</td>\n",
       "      <td>1.241203</td>\n",
       "      <td>47.080746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_3</th>\n",
       "      <td>8975.0</td>\n",
       "      <td>-0.084745</td>\n",
       "      <td>0.913747</td>\n",
       "      <td>-1.935016</td>\n",
       "      <td>-1.135189</td>\n",
       "      <td>-0.354605</td>\n",
       "      <td>3.845363</td>\n",
       "      <td>10.215664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_112</th>\n",
       "      <td>11078.0</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>0.986731</td>\n",
       "      <td>-2.342529</td>\n",
       "      <td>-2.337321</td>\n",
       "      <td>0.428811</td>\n",
       "      <td>0.444948</td>\n",
       "      <td>0.445272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_4</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.023498</td>\n",
       "      <td>0.975719</td>\n",
       "      <td>-0.773477</td>\n",
       "      <td>-0.770625</td>\n",
       "      <td>-0.421431</td>\n",
       "      <td>3.728115</td>\n",
       "      <td>8.575579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_51</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>0.013309</td>\n",
       "      <td>1.020575</td>\n",
       "      <td>-0.591766</td>\n",
       "      <td>-0.591212</td>\n",
       "      <td>-0.561347</td>\n",
       "      <td>3.632092</td>\n",
       "      <td>7.831568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_23</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.066105</td>\n",
       "      <td>0.940107</td>\n",
       "      <td>-0.748060</td>\n",
       "      <td>-0.742494</td>\n",
       "      <td>-0.529887</td>\n",
       "      <td>3.368483</td>\n",
       "      <td>5.212445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_64_O</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.010049</td>\n",
       "      <td>1.000319</td>\n",
       "      <td>-1.032779</td>\n",
       "      <td>-1.032779</td>\n",
       "      <td>0.968262</td>\n",
       "      <td>0.968262</td>\n",
       "      <td>0.968262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_50</th>\n",
       "      <td>4830.0</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>0.973135</td>\n",
       "      <td>-2.543185</td>\n",
       "      <td>-0.453001</td>\n",
       "      <td>-0.162385</td>\n",
       "      <td>2.357418</td>\n",
       "      <td>31.209787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_2</th>\n",
       "      <td>10941.0</td>\n",
       "      <td>0.093845</td>\n",
       "      <td>0.950313</td>\n",
       "      <td>-3.455269</td>\n",
       "      <td>-2.376498</td>\n",
       "      <td>0.247144</td>\n",
       "      <td>1.458669</td>\n",
       "      <td>1.474695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_41</th>\n",
       "      <td>11078.0</td>\n",
       "      <td>-0.021021</td>\n",
       "      <td>0.900287</td>\n",
       "      <td>-0.287878</td>\n",
       "      <td>-0.287255</td>\n",
       "      <td>-0.257975</td>\n",
       "      <td>4.218818</td>\n",
       "      <td>17.855449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_47</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>0.080265</td>\n",
       "      <td>1.006937</td>\n",
       "      <td>-1.807211</td>\n",
       "      <td>-1.725752</td>\n",
       "      <td>-0.001881</td>\n",
       "      <td>2.587299</td>\n",
       "      <td>4.077495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_3</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.071010</td>\n",
       "      <td>0.923865</td>\n",
       "      <td>-0.578135</td>\n",
       "      <td>-0.577341</td>\n",
       "      <td>-0.537084</td>\n",
       "      <td>3.422853</td>\n",
       "      <td>29.064712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_43</th>\n",
       "      <td>7639.0</td>\n",
       "      <td>-0.055412</td>\n",
       "      <td>0.935598</td>\n",
       "      <td>-0.708862</td>\n",
       "      <td>-0.698513</td>\n",
       "      <td>-0.338943</td>\n",
       "      <td>3.530841</td>\n",
       "      <td>22.671087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_1</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.089938</td>\n",
       "      <td>0.820015</td>\n",
       "      <td>-0.348943</td>\n",
       "      <td>-0.348422</td>\n",
       "      <td>-0.323690</td>\n",
       "      <td>4.190355</td>\n",
       "      <td>8.744553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_42</th>\n",
       "      <td>1620.0</td>\n",
       "      <td>-0.031186</td>\n",
       "      <td>0.816427</td>\n",
       "      <td>-0.765913</td>\n",
       "      <td>-0.755756</td>\n",
       "      <td>-0.282781</td>\n",
       "      <td>2.851695</td>\n",
       "      <td>5.568950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_26</th>\n",
       "      <td>1082.0</td>\n",
       "      <td>-0.007377</td>\n",
       "      <td>1.188077</td>\n",
       "      <td>-0.341708</td>\n",
       "      <td>-0.341305</td>\n",
       "      <td>-0.193408</td>\n",
       "      <td>3.323656</td>\n",
       "      <td>26.289284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_45</th>\n",
       "      <td>11078.0</td>\n",
       "      <td>0.092365</td>\n",
       "      <td>1.006858</td>\n",
       "      <td>-1.001841</td>\n",
       "      <td>-0.985800</td>\n",
       "      <td>-0.074423</td>\n",
       "      <td>3.204536</td>\n",
       "      <td>5.328335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_9</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.077388</td>\n",
       "      <td>0.965691</td>\n",
       "      <td>-0.665986</td>\n",
       "      <td>-0.665140</td>\n",
       "      <td>-0.604840</td>\n",
       "      <td>2.725575</td>\n",
       "      <td>21.489204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_3</th>\n",
       "      <td>11078.0</td>\n",
       "      <td>-0.062772</td>\n",
       "      <td>0.923082</td>\n",
       "      <td>-0.551970</td>\n",
       "      <td>-0.550923</td>\n",
       "      <td>-0.511790</td>\n",
       "      <td>3.632044</td>\n",
       "      <td>4.906471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_66_1.0</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.004265</td>\n",
       "      <td>0.994660</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>2.863369</td>\n",
       "      <td>2.863369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_27</th>\n",
       "      <td>10953.0</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.923831</td>\n",
       "      <td>-2.827796</td>\n",
       "      <td>-2.769465</td>\n",
       "      <td>0.363672</td>\n",
       "      <td>0.380874</td>\n",
       "      <td>0.381243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_61</th>\n",
       "      <td>9822.0</td>\n",
       "      <td>-0.057740</td>\n",
       "      <td>0.995428</td>\n",
       "      <td>-1.305017</td>\n",
       "      <td>-1.276828</td>\n",
       "      <td>-0.273995</td>\n",
       "      <td>1.628758</td>\n",
       "      <td>11.870492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_7</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.069335</td>\n",
       "      <td>0.939926</td>\n",
       "      <td>-0.971978</td>\n",
       "      <td>-0.797173</td>\n",
       "      <td>-0.528294</td>\n",
       "      <td>3.274028</td>\n",
       "      <td>4.567728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_132</th>\n",
       "      <td>937.0</td>\n",
       "      <td>0.024820</td>\n",
       "      <td>1.087154</td>\n",
       "      <td>-0.868249</td>\n",
       "      <td>-0.825159</td>\n",
       "      <td>-0.208652</td>\n",
       "      <td>4.445321</td>\n",
       "      <td>14.635735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_46</th>\n",
       "      <td>8490.0</td>\n",
       "      <td>-0.025886</td>\n",
       "      <td>0.914897</td>\n",
       "      <td>-16.342927</td>\n",
       "      <td>-2.656341</td>\n",
       "      <td>-0.112862</td>\n",
       "      <td>2.781258</td>\n",
       "      <td>8.307961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_1</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.080050</td>\n",
       "      <td>0.902951</td>\n",
       "      <td>-1.084784</td>\n",
       "      <td>-0.583364</td>\n",
       "      <td>-0.459123</td>\n",
       "      <td>3.772038</td>\n",
       "      <td>5.627681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_49</th>\n",
       "      <td>937.0</td>\n",
       "      <td>-0.032667</td>\n",
       "      <td>0.853130</td>\n",
       "      <td>-0.748312</td>\n",
       "      <td>-0.734794</td>\n",
       "      <td>-0.240521</td>\n",
       "      <td>2.261374</td>\n",
       "      <td>16.027758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_5</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.005996</td>\n",
       "      <td>0.763669</td>\n",
       "      <td>-0.255185</td>\n",
       "      <td>-0.253910</td>\n",
       "      <td>-0.205255</td>\n",
       "      <td>2.601373</td>\n",
       "      <td>21.070905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_39</th>\n",
       "      <td>11082.0</td>\n",
       "      <td>-0.031308</td>\n",
       "      <td>0.978555</td>\n",
       "      <td>-0.586205</td>\n",
       "      <td>-0.585477</td>\n",
       "      <td>-0.553863</td>\n",
       "      <td>3.241953</td>\n",
       "      <td>18.431782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_7</th>\n",
       "      <td>8975.0</td>\n",
       "      <td>-0.077454</td>\n",
       "      <td>0.937544</td>\n",
       "      <td>-1.562081</td>\n",
       "      <td>-1.061203</td>\n",
       "      <td>-0.471512</td>\n",
       "      <td>3.646149</td>\n",
       "      <td>8.677838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count      mean       std        min        1%       50%  \\\n",
       "D_62       9467.0  0.024106  1.025060  -0.844205 -0.816770 -0.378554   \n",
       "D_75      11082.0 -0.040425  0.976330  -0.765714 -0.764696 -0.436095   \n",
       "B_2       11078.0  0.056946  0.977543  -1.566291 -1.556970  0.478706   \n",
       "B_6       11082.0 -0.003627  0.727327  -0.219524 -0.212509 -0.081762   \n",
       "S_3        8975.0 -0.084745  0.913747  -1.935016 -1.135189 -0.354605   \n",
       "D_112     11078.0  0.013930  0.986731  -2.342529 -2.337321  0.428811   \n",
       "B_4       11082.0 -0.023498  0.975719  -0.773477 -0.770625 -0.421431   \n",
       "D_51      11082.0  0.013309  1.020575  -0.591766 -0.591212 -0.561347   \n",
       "B_23      11082.0 -0.066105  0.940107  -0.748060 -0.742494 -0.529887   \n",
       "D_64_O    11082.0 -0.010049  1.000319  -1.032779 -1.032779  0.968262   \n",
       "D_50       4830.0  0.009335  0.973135  -2.543185 -0.453001 -0.162385   \n",
       "P_2       10941.0  0.093845  0.950313  -3.455269 -2.376498  0.247144   \n",
       "D_41      11078.0 -0.021021  0.900287  -0.287878 -0.287255 -0.257975   \n",
       "D_47      11082.0  0.080265  1.006937  -1.807211 -1.725752 -0.001881   \n",
       "R_3       11082.0 -0.071010  0.923865  -0.578135 -0.577341 -0.537084   \n",
       "D_43       7639.0 -0.055412  0.935598  -0.708862 -0.698513 -0.338943   \n",
       "R_1       11082.0 -0.089938  0.820015  -0.348943 -0.348422 -0.323690   \n",
       "D_42       1620.0 -0.031186  0.816427  -0.765913 -0.755756 -0.282781   \n",
       "R_26       1082.0 -0.007377  1.188077  -0.341708 -0.341305 -0.193408   \n",
       "D_45      11078.0  0.092365  1.006858  -1.001841 -0.985800 -0.074423   \n",
       "B_9       11082.0 -0.077388  0.965691  -0.665986 -0.665140 -0.604840   \n",
       "B_3       11078.0 -0.062772  0.923082  -0.551970 -0.550923 -0.511790   \n",
       "D_66_1.0  11082.0 -0.004265  0.994660  -0.349239 -0.349239 -0.349239   \n",
       "R_27      10953.0  0.059800  0.923831  -2.827796 -2.769465  0.363672   \n",
       "D_61       9822.0 -0.057740  0.995428  -1.305017 -1.276828 -0.273995   \n",
       "B_7       11082.0 -0.069335  0.939926  -0.971978 -0.797173 -0.528294   \n",
       "D_132       937.0  0.024820  1.087154  -0.868249 -0.825159 -0.208652   \n",
       "D_46       8490.0 -0.025886  0.914897 -16.342927 -2.656341 -0.112862   \n",
       "B_1       11082.0 -0.080050  0.902951  -1.084784 -0.583364 -0.459123   \n",
       "D_49        937.0 -0.032667  0.853130  -0.748312 -0.734794 -0.240521   \n",
       "B_5       11082.0 -0.005996  0.763669  -0.255185 -0.253910 -0.205255   \n",
       "D_39      11082.0 -0.031308  0.978555  -0.586205 -0.585477 -0.553863   \n",
       "S_7        8975.0 -0.077454  0.937544  -1.562081 -1.061203 -0.471512   \n",
       "\n",
       "               99%        max  \n",
       "D_62      3.580413  23.525745  \n",
       "D_75      3.476083   8.875216  \n",
       "B_2       0.967644   0.968316  \n",
       "B_6       1.241203  47.080746  \n",
       "S_3       3.845363  10.215664  \n",
       "D_112     0.444948   0.445272  \n",
       "B_4       3.728115   8.575579  \n",
       "D_51      3.632092   7.831568  \n",
       "B_23      3.368483   5.212445  \n",
       "D_64_O    0.968262   0.968262  \n",
       "D_50      2.357418  31.209787  \n",
       "P_2       1.458669   1.474695  \n",
       "D_41      4.218818  17.855449  \n",
       "D_47      2.587299   4.077495  \n",
       "R_3       3.422853  29.064712  \n",
       "D_43      3.530841  22.671087  \n",
       "R_1       4.190355   8.744553  \n",
       "D_42      2.851695   5.568950  \n",
       "R_26      3.323656  26.289284  \n",
       "D_45      3.204536   5.328335  \n",
       "B_9       2.725575  21.489204  \n",
       "B_3       3.632044   4.906471  \n",
       "D_66_1.0  2.863369   2.863369  \n",
       "R_27      0.380874   0.381243  \n",
       "D_61      1.628758  11.870492  \n",
       "B_7       3.274028   4.567728  \n",
       "D_132     4.445321  14.635735  \n",
       "D_46      2.781258   8.307961  \n",
       "B_1       3.772038   5.627681  \n",
       "D_49      2.261374  16.027758  \n",
       "B_5       2.601373  21.070905  \n",
       "D_39      3.241953  18.431782  \n",
       "S_7       3.646149   8.677838  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_normalized.describe(percentiles=[0.01,0.99]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b58714de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>50%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D_62</th>\n",
       "      <td>13866.0</td>\n",
       "      <td>-0.048811</td>\n",
       "      <td>0.988677</td>\n",
       "      <td>-0.845349</td>\n",
       "      <td>-0.824784</td>\n",
       "      <td>-0.490066</td>\n",
       "      <td>3.390996</td>\n",
       "      <td>13.789451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_75</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>0.044823</td>\n",
       "      <td>1.041162</td>\n",
       "      <td>-0.765712</td>\n",
       "      <td>-0.764454</td>\n",
       "      <td>-0.425997</td>\n",
       "      <td>3.774501</td>\n",
       "      <td>9.781580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_2</th>\n",
       "      <td>16055.0</td>\n",
       "      <td>-0.089347</td>\n",
       "      <td>1.026670</td>\n",
       "      <td>-1.566369</td>\n",
       "      <td>-1.562321</td>\n",
       "      <td>0.474697</td>\n",
       "      <td>0.967429</td>\n",
       "      <td>0.968317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_6</th>\n",
       "      <td>16054.0</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>1.011876</td>\n",
       "      <td>-0.221541</td>\n",
       "      <td>-0.213567</td>\n",
       "      <td>-0.085343</td>\n",
       "      <td>1.708769</td>\n",
       "      <td>49.640796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_3</th>\n",
       "      <td>13163.0</td>\n",
       "      <td>0.039034</td>\n",
       "      <td>1.009385</td>\n",
       "      <td>-2.385599</td>\n",
       "      <td>-1.133564</td>\n",
       "      <td>-0.312218</td>\n",
       "      <td>3.935197</td>\n",
       "      <td>14.010590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_112</th>\n",
       "      <td>16055.0</td>\n",
       "      <td>-0.051896</td>\n",
       "      <td>1.047377</td>\n",
       "      <td>-2.342666</td>\n",
       "      <td>-2.339219</td>\n",
       "      <td>0.428478</td>\n",
       "      <td>0.444903</td>\n",
       "      <td>0.445272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_4</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>0.039085</td>\n",
       "      <td>1.018012</td>\n",
       "      <td>-0.773482</td>\n",
       "      <td>-0.770325</td>\n",
       "      <td>-0.364603</td>\n",
       "      <td>3.842134</td>\n",
       "      <td>11.568722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_51</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>-0.054241</td>\n",
       "      <td>0.961936</td>\n",
       "      <td>-0.591777</td>\n",
       "      <td>-0.591221</td>\n",
       "      <td>-0.562872</td>\n",
       "      <td>3.627147</td>\n",
       "      <td>9.241091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_23</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>0.092795</td>\n",
       "      <td>1.084639</td>\n",
       "      <td>-0.748127</td>\n",
       "      <td>-0.741575</td>\n",
       "      <td>-0.415311</td>\n",
       "      <td>3.986888</td>\n",
       "      <td>5.259023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_64_O</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>-0.080276</td>\n",
       "      <td>0.999398</td>\n",
       "      <td>-1.032779</td>\n",
       "      <td>-1.032779</td>\n",
       "      <td>-1.032779</td>\n",
       "      <td>0.968262</td>\n",
       "      <td>0.968262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_50</th>\n",
       "      <td>6405.0</td>\n",
       "      <td>-0.011839</td>\n",
       "      <td>1.043591</td>\n",
       "      <td>-1.938961</td>\n",
       "      <td>-0.455161</td>\n",
       "      <td>-0.171628</td>\n",
       "      <td>2.169405</td>\n",
       "      <td>54.423823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_2</th>\n",
       "      <td>15842.0</td>\n",
       "      <td>-0.141090</td>\n",
       "      <td>1.070202</td>\n",
       "      <td>-4.087693</td>\n",
       "      <td>-2.920226</td>\n",
       "      <td>-0.024673</td>\n",
       "      <td>1.452284</td>\n",
       "      <td>1.474685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_41</th>\n",
       "      <td>16055.0</td>\n",
       "      <td>0.159158</td>\n",
       "      <td>1.404788</td>\n",
       "      <td>-0.287878</td>\n",
       "      <td>-0.287283</td>\n",
       "      <td>-0.256396</td>\n",
       "      <td>7.043585</td>\n",
       "      <td>20.375166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_47</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>-0.106066</td>\n",
       "      <td>0.991249</td>\n",
       "      <td>-1.806694</td>\n",
       "      <td>-1.734607</td>\n",
       "      <td>-0.228243</td>\n",
       "      <td>2.514502</td>\n",
       "      <td>4.181887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_3</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>0.058029</td>\n",
       "      <td>1.033838</td>\n",
       "      <td>-0.578153</td>\n",
       "      <td>-0.577185</td>\n",
       "      <td>-0.130668</td>\n",
       "      <td>4.288294</td>\n",
       "      <td>20.248397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_43</th>\n",
       "      <td>10140.0</td>\n",
       "      <td>0.059869</td>\n",
       "      <td>1.096219</td>\n",
       "      <td>-0.708957</td>\n",
       "      <td>-0.698017</td>\n",
       "      <td>-0.286614</td>\n",
       "      <td>4.301885</td>\n",
       "      <td>29.787737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_1</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>0.119006</td>\n",
       "      <td>1.214961</td>\n",
       "      <td>-0.348942</td>\n",
       "      <td>-0.348384</td>\n",
       "      <td>-0.322071</td>\n",
       "      <td>5.350496</td>\n",
       "      <td>11.006346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_42</th>\n",
       "      <td>4036.0</td>\n",
       "      <td>-0.001181</td>\n",
       "      <td>1.141386</td>\n",
       "      <td>-0.765402</td>\n",
       "      <td>-0.750858</td>\n",
       "      <td>-0.329397</td>\n",
       "      <td>4.274020</td>\n",
       "      <td>16.403768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_26</th>\n",
       "      <td>1586.0</td>\n",
       "      <td>0.068924</td>\n",
       "      <td>1.264883</td>\n",
       "      <td>-0.341683</td>\n",
       "      <td>-0.340503</td>\n",
       "      <td>-0.191993</td>\n",
       "      <td>5.787731</td>\n",
       "      <td>19.026968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_45</th>\n",
       "      <td>16055.0</td>\n",
       "      <td>-0.160982</td>\n",
       "      <td>0.965405</td>\n",
       "      <td>-1.001747</td>\n",
       "      <td>-0.994140</td>\n",
       "      <td>-0.605318</td>\n",
       "      <td>3.048995</td>\n",
       "      <td>5.419426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_9</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>0.079396</td>\n",
       "      <td>1.035297</td>\n",
       "      <td>-0.665995</td>\n",
       "      <td>-0.665187</td>\n",
       "      <td>-0.518203</td>\n",
       "      <td>2.840037</td>\n",
       "      <td>24.228726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_3</th>\n",
       "      <td>16055.0</td>\n",
       "      <td>0.065840</td>\n",
       "      <td>1.060669</td>\n",
       "      <td>-0.551971</td>\n",
       "      <td>-0.550961</td>\n",
       "      <td>-0.509584</td>\n",
       "      <td>3.801278</td>\n",
       "      <td>5.628230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_66_1.0</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>-0.003281</td>\n",
       "      <td>0.995892</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>2.863369</td>\n",
       "      <td>2.863369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_27</th>\n",
       "      <td>12709.0</td>\n",
       "      <td>-0.021459</td>\n",
       "      <td>1.024926</td>\n",
       "      <td>-2.833366</td>\n",
       "      <td>-2.778455</td>\n",
       "      <td>0.363023</td>\n",
       "      <td>0.380932</td>\n",
       "      <td>0.381244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_61</th>\n",
       "      <td>14381.0</td>\n",
       "      <td>0.082419</td>\n",
       "      <td>1.011736</td>\n",
       "      <td>-1.304960</td>\n",
       "      <td>-1.274760</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>1.767873</td>\n",
       "      <td>7.484149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_7</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>0.098385</td>\n",
       "      <td>1.089180</td>\n",
       "      <td>-1.203183</td>\n",
       "      <td>-0.796203</td>\n",
       "      <td>-0.393874</td>\n",
       "      <td>4.030673</td>\n",
       "      <td>4.567826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_132</th>\n",
       "      <td>1792.0</td>\n",
       "      <td>0.009713</td>\n",
       "      <td>1.067949</td>\n",
       "      <td>-0.854764</td>\n",
       "      <td>-0.814724</td>\n",
       "      <td>-0.220482</td>\n",
       "      <td>3.510902</td>\n",
       "      <td>18.117073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_46</th>\n",
       "      <td>10858.0</td>\n",
       "      <td>0.023128</td>\n",
       "      <td>1.147775</td>\n",
       "      <td>-12.668007</td>\n",
       "      <td>-2.980574</td>\n",
       "      <td>-0.083098</td>\n",
       "      <td>3.272937</td>\n",
       "      <td>40.845096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_1</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>0.122451</td>\n",
       "      <td>1.152854</td>\n",
       "      <td>-1.042212</td>\n",
       "      <td>-0.583152</td>\n",
       "      <td>-0.400411</td>\n",
       "      <td>4.997259</td>\n",
       "      <td>5.627832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_49</th>\n",
       "      <td>1792.0</td>\n",
       "      <td>-0.003934</td>\n",
       "      <td>0.794131</td>\n",
       "      <td>-0.748549</td>\n",
       "      <td>-0.733425</td>\n",
       "      <td>-0.236082</td>\n",
       "      <td>2.864303</td>\n",
       "      <td>9.559399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_5</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>-0.029735</td>\n",
       "      <td>0.815435</td>\n",
       "      <td>-0.255189</td>\n",
       "      <td>-0.254024</td>\n",
       "      <td>-0.212643</td>\n",
       "      <td>2.501728</td>\n",
       "      <td>36.087264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_39</th>\n",
       "      <td>16065.0</td>\n",
       "      <td>0.179644</td>\n",
       "      <td>1.465573</td>\n",
       "      <td>-0.586204</td>\n",
       "      <td>-0.585346</td>\n",
       "      <td>-0.546935</td>\n",
       "      <td>6.738429</td>\n",
       "      <td>12.843235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_7</th>\n",
       "      <td>13163.0</td>\n",
       "      <td>0.031068</td>\n",
       "      <td>1.004177</td>\n",
       "      <td>-1.950135</td>\n",
       "      <td>-1.053319</td>\n",
       "      <td>-0.330692</td>\n",
       "      <td>3.809475</td>\n",
       "      <td>10.206350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count      mean       std        min        1%       50%  \\\n",
       "D_62      13866.0 -0.048811  0.988677  -0.845349 -0.824784 -0.490066   \n",
       "D_75      16065.0  0.044823  1.041162  -0.765712 -0.764454 -0.425997   \n",
       "B_2       16055.0 -0.089347  1.026670  -1.566369 -1.562321  0.474697   \n",
       "B_6       16054.0  0.025084  1.011876  -0.221541 -0.213567 -0.085343   \n",
       "S_3       13163.0  0.039034  1.009385  -2.385599 -1.133564 -0.312218   \n",
       "D_112     16055.0 -0.051896  1.047377  -2.342666 -2.339219  0.428478   \n",
       "B_4       16065.0  0.039085  1.018012  -0.773482 -0.770325 -0.364603   \n",
       "D_51      16065.0 -0.054241  0.961936  -0.591777 -0.591221 -0.562872   \n",
       "B_23      16065.0  0.092795  1.084639  -0.748127 -0.741575 -0.415311   \n",
       "D_64_O    16065.0 -0.080276  0.999398  -1.032779 -1.032779 -1.032779   \n",
       "D_50       6405.0 -0.011839  1.043591  -1.938961 -0.455161 -0.171628   \n",
       "P_2       15842.0 -0.141090  1.070202  -4.087693 -2.920226 -0.024673   \n",
       "D_41      16055.0  0.159158  1.404788  -0.287878 -0.287283 -0.256396   \n",
       "D_47      16065.0 -0.106066  0.991249  -1.806694 -1.734607 -0.228243   \n",
       "R_3       16065.0  0.058029  1.033838  -0.578153 -0.577185 -0.130668   \n",
       "D_43      10140.0  0.059869  1.096219  -0.708957 -0.698017 -0.286614   \n",
       "R_1       16065.0  0.119006  1.214961  -0.348942 -0.348384 -0.322071   \n",
       "D_42       4036.0 -0.001181  1.141386  -0.765402 -0.750858 -0.329397   \n",
       "R_26       1586.0  0.068924  1.264883  -0.341683 -0.340503 -0.191993   \n",
       "D_45      16055.0 -0.160982  0.965405  -1.001747 -0.994140 -0.605318   \n",
       "B_9       16065.0  0.079396  1.035297  -0.665995 -0.665187 -0.518203   \n",
       "B_3       16055.0  0.065840  1.060669  -0.551971 -0.550961 -0.509584   \n",
       "D_66_1.0  16065.0 -0.003281  0.995892  -0.349239 -0.349239 -0.349239   \n",
       "R_27      12709.0 -0.021459  1.024926  -2.833366 -2.778455  0.363023   \n",
       "D_61      14381.0  0.082419  1.011736  -1.304960 -1.274760  0.003841   \n",
       "B_7       16065.0  0.098385  1.089180  -1.203183 -0.796203 -0.393874   \n",
       "D_132      1792.0  0.009713  1.067949  -0.854764 -0.814724 -0.220482   \n",
       "D_46      10858.0  0.023128  1.147775 -12.668007 -2.980574 -0.083098   \n",
       "B_1       16065.0  0.122451  1.152854  -1.042212 -0.583152 -0.400411   \n",
       "D_49       1792.0 -0.003934  0.794131  -0.748549 -0.733425 -0.236082   \n",
       "B_5       16065.0 -0.029735  0.815435  -0.255189 -0.254024 -0.212643   \n",
       "D_39      16065.0  0.179644  1.465573  -0.586204 -0.585346 -0.546935   \n",
       "S_7       13163.0  0.031068  1.004177  -1.950135 -1.053319 -0.330692   \n",
       "\n",
       "               99%        max  \n",
       "D_62      3.390996  13.789451  \n",
       "D_75      3.774501   9.781580  \n",
       "B_2       0.967429   0.968317  \n",
       "B_6       1.708769  49.640796  \n",
       "S_3       3.935197  14.010590  \n",
       "D_112     0.444903   0.445272  \n",
       "B_4       3.842134  11.568722  \n",
       "D_51      3.627147   9.241091  \n",
       "B_23      3.986888   5.259023  \n",
       "D_64_O    0.968262   0.968262  \n",
       "D_50      2.169405  54.423823  \n",
       "P_2       1.452284   1.474685  \n",
       "D_41      7.043585  20.375166  \n",
       "D_47      2.514502   4.181887  \n",
       "R_3       4.288294  20.248397  \n",
       "D_43      4.301885  29.787737  \n",
       "R_1       5.350496  11.006346  \n",
       "D_42      4.274020  16.403768  \n",
       "R_26      5.787731  19.026968  \n",
       "D_45      3.048995   5.419426  \n",
       "B_9       2.840037  24.228726  \n",
       "B_3       3.801278   5.628230  \n",
       "D_66_1.0  2.863369   2.863369  \n",
       "R_27      0.380932   0.381244  \n",
       "D_61      1.767873   7.484149  \n",
       "B_7       4.030673   4.567826  \n",
       "D_132     3.510902  18.117073  \n",
       "D_46      3.272937  40.845096  \n",
       "B_1       4.997259   5.627832  \n",
       "D_49      2.864303   9.559399  \n",
       "B_5       2.501728  36.087264  \n",
       "D_39      6.738429  12.843235  \n",
       "S_7       3.809475  10.206350  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1_normalized.describe(percentiles=[0.01,0.99]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2da95faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 1st and 99th percentile values for each column\n",
    "p1 = X_train_normalized.quantile(0.01)\n",
    "p99 = X_train_normalized.quantile(0.99)\n",
    "\n",
    "# Replace values outside the percentile range with the percentile value for each column\n",
    "X_train_normalized = X_train_normalized.apply(lambda x: nm.where(x < p1[x.name], p1[x.name], x))\n",
    "X_train_normalized = X_train_normalized.apply(lambda x: nm.where(x > p99[x.name], p99[x.name], x))\n",
    "\n",
    "# Apply the same operation to the test data\n",
    "X_test_normalized = X_test_normalized.apply(lambda x: nm.where(x < p1[x.name], p1[x.name], x))\n",
    "X_test_normalized = X_test_normalized.apply(lambda x: nm.where(x > p99[x.name], p99[x.name], x))\n",
    "\n",
    "#Apply to test2 data\n",
    "X_test1_normalized = X_test1_normalized.apply(lambda x: nm.where(x < p1[x.name], p1[x.name], x))\n",
    "X_test1_normalized = X_test1_normalized.apply(lambda x: nm.where(x > p99[x.name], p99[x.name], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "de2bdb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>50%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D_62</th>\n",
       "      <td>48189.0</td>\n",
       "      <td>-1.060948e-02</td>\n",
       "      <td>0.944455</td>\n",
       "      <td>-0.820717</td>\n",
       "      <td>-0.820714</td>\n",
       "      <td>-0.430038</td>\n",
       "      <td>3.638186</td>\n",
       "      <td>3.638394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_75</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-1.089630e-02</td>\n",
       "      <td>0.943937</td>\n",
       "      <td>-0.764539</td>\n",
       "      <td>-0.764539</td>\n",
       "      <td>-0.430422</td>\n",
       "      <td>3.754295</td>\n",
       "      <td>3.754376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_2</th>\n",
       "      <td>55779.0</td>\n",
       "      <td>3.663197e-05</td>\n",
       "      <td>0.999942</td>\n",
       "      <td>-1.558451</td>\n",
       "      <td>-1.558449</td>\n",
       "      <td>0.477259</td>\n",
       "      <td>0.967569</td>\n",
       "      <td>0.967569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_6</th>\n",
       "      <td>55818.0</td>\n",
       "      <td>-3.361485e-02</td>\n",
       "      <td>0.220877</td>\n",
       "      <td>-0.213237</td>\n",
       "      <td>-0.213237</td>\n",
       "      <td>-0.093159</td>\n",
       "      <td>1.285047</td>\n",
       "      <td>1.285272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_3</th>\n",
       "      <td>45622.0</td>\n",
       "      <td>-1.135974e-02</td>\n",
       "      <td>0.922493</td>\n",
       "      <td>-1.139953</td>\n",
       "      <td>-1.139902</td>\n",
       "      <td>-0.332569</td>\n",
       "      <td>3.996685</td>\n",
       "      <td>3.997569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_112</th>\n",
       "      <td>55766.0</td>\n",
       "      <td>2.124820e-05</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>-2.337388</td>\n",
       "      <td>-2.337386</td>\n",
       "      <td>0.428788</td>\n",
       "      <td>0.444942</td>\n",
       "      <td>0.444942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_4</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-1.155608e-02</td>\n",
       "      <td>0.939662</td>\n",
       "      <td>-0.770369</td>\n",
       "      <td>-0.770365</td>\n",
       "      <td>-0.402848</td>\n",
       "      <td>3.761493</td>\n",
       "      <td>3.761549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_51</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-7.955249e-03</td>\n",
       "      <td>0.962945</td>\n",
       "      <td>-0.591157</td>\n",
       "      <td>-0.591157</td>\n",
       "      <td>-0.561613</td>\n",
       "      <td>3.631421</td>\n",
       "      <td>3.631422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_23</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-5.240230e-03</td>\n",
       "      <td>0.978074</td>\n",
       "      <td>-0.742303</td>\n",
       "      <td>-0.742303</td>\n",
       "      <td>-0.483066</td>\n",
       "      <td>3.703590</td>\n",
       "      <td>3.703861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_64_O</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>6.770952e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-1.032779</td>\n",
       "      <td>-1.032779</td>\n",
       "      <td>0.968262</td>\n",
       "      <td>0.968262</td>\n",
       "      <td>0.968262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_50</th>\n",
       "      <td>23940.0</td>\n",
       "      <td>-3.711052e-02</td>\n",
       "      <td>0.424495</td>\n",
       "      <td>-0.454422</td>\n",
       "      <td>-0.454408</td>\n",
       "      <td>-0.164277</td>\n",
       "      <td>2.244762</td>\n",
       "      <td>2.245851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_2</th>\n",
       "      <td>55155.0</td>\n",
       "      <td>3.242735e-03</td>\n",
       "      <td>0.990101</td>\n",
       "      <td>-2.634398</td>\n",
       "      <td>-2.634383</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>1.456492</td>\n",
       "      <td>1.456492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_41</th>\n",
       "      <td>55779.0</td>\n",
       "      <td>-2.611066e-02</td>\n",
       "      <td>0.775114</td>\n",
       "      <td>-0.287232</td>\n",
       "      <td>-0.287232</td>\n",
       "      <td>-0.257311</td>\n",
       "      <td>4.441462</td>\n",
       "      <td>4.442563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_47</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-2.314265e-03</td>\n",
       "      <td>0.990833</td>\n",
       "      <td>-1.714800</td>\n",
       "      <td>-1.714797</td>\n",
       "      <td>-0.101906</td>\n",
       "      <td>2.538762</td>\n",
       "      <td>2.539341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_3</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-2.120974e-02</td>\n",
       "      <td>0.840382</td>\n",
       "      <td>-0.577217</td>\n",
       "      <td>-0.577217</td>\n",
       "      <td>-0.534926</td>\n",
       "      <td>3.875588</td>\n",
       "      <td>3.875620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_43</th>\n",
       "      <td>38392.0</td>\n",
       "      <td>-2.287668e-02</td>\n",
       "      <td>0.810595</td>\n",
       "      <td>-0.697607</td>\n",
       "      <td>-0.697599</td>\n",
       "      <td>-0.304335</td>\n",
       "      <td>3.895190</td>\n",
       "      <td>3.895852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_1</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-1.686142e-02</td>\n",
       "      <td>0.901345</td>\n",
       "      <td>-0.348424</td>\n",
       "      <td>-0.348424</td>\n",
       "      <td>-0.322585</td>\n",
       "      <td>4.223875</td>\n",
       "      <td>4.223893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_42</th>\n",
       "      <td>9854.0</td>\n",
       "      <td>-2.790335e-02</td>\n",
       "      <td>0.789411</td>\n",
       "      <td>-0.753075</td>\n",
       "      <td>-0.753066</td>\n",
       "      <td>-0.261952</td>\n",
       "      <td>3.294024</td>\n",
       "      <td>3.296312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_26</th>\n",
       "      <td>6402.0</td>\n",
       "      <td>-4.036880e-02</td>\n",
       "      <td>0.574373</td>\n",
       "      <td>-0.340933</td>\n",
       "      <td>-0.340933</td>\n",
       "      <td>-0.191259</td>\n",
       "      <td>3.438385</td>\n",
       "      <td>3.439473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_45</th>\n",
       "      <td>55779.0</td>\n",
       "      <td>-5.634861e-03</td>\n",
       "      <td>0.979098</td>\n",
       "      <td>-0.989321</td>\n",
       "      <td>-0.989315</td>\n",
       "      <td>-0.323737</td>\n",
       "      <td>3.114217</td>\n",
       "      <td>3.114469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_9</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-1.679233e-02</td>\n",
       "      <td>0.881581</td>\n",
       "      <td>-0.665134</td>\n",
       "      <td>-0.665133</td>\n",
       "      <td>-0.567922</td>\n",
       "      <td>2.742118</td>\n",
       "      <td>2.742730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_3</th>\n",
       "      <td>55779.0</td>\n",
       "      <td>-4.624272e-03</td>\n",
       "      <td>0.980626</td>\n",
       "      <td>-0.550884</td>\n",
       "      <td>-0.550884</td>\n",
       "      <td>-0.510392</td>\n",
       "      <td>3.763595</td>\n",
       "      <td>3.763759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_66_1.0</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>2.278196e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>-0.349239</td>\n",
       "      <td>2.863369</td>\n",
       "      <td>2.863369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_27</th>\n",
       "      <td>54269.0</td>\n",
       "      <td>1.335507e-04</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>-2.776491</td>\n",
       "      <td>-2.776489</td>\n",
       "      <td>0.363459</td>\n",
       "      <td>0.380876</td>\n",
       "      <td>0.380876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_61</th>\n",
       "      <td>49899.0</td>\n",
       "      <td>-4.871603e-03</td>\n",
       "      <td>0.986744</td>\n",
       "      <td>-1.275128</td>\n",
       "      <td>-1.275127</td>\n",
       "      <td>-0.154373</td>\n",
       "      <td>1.687814</td>\n",
       "      <td>1.687847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_7</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-5.891742e-03</td>\n",
       "      <td>0.975613</td>\n",
       "      <td>-0.798153</td>\n",
       "      <td>-0.798150</td>\n",
       "      <td>-0.472853</td>\n",
       "      <td>3.579867</td>\n",
       "      <td>3.579970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_132</th>\n",
       "      <td>5484.0</td>\n",
       "      <td>-3.794935e-02</td>\n",
       "      <td>0.685445</td>\n",
       "      <td>-0.828160</td>\n",
       "      <td>-0.828113</td>\n",
       "      <td>-0.185995</td>\n",
       "      <td>2.977135</td>\n",
       "      <td>2.977399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_46</th>\n",
       "      <td>42258.0</td>\n",
       "      <td>-1.186074e-03</td>\n",
       "      <td>0.804484</td>\n",
       "      <td>-2.749854</td>\n",
       "      <td>-2.749630</td>\n",
       "      <td>-0.091835</td>\n",
       "      <td>3.119942</td>\n",
       "      <td>3.120153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_1</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-7.529544e-03</td>\n",
       "      <td>0.961959</td>\n",
       "      <td>-0.583203</td>\n",
       "      <td>-0.583200</td>\n",
       "      <td>-0.432805</td>\n",
       "      <td>4.198606</td>\n",
       "      <td>4.198745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_49</th>\n",
       "      <td>5533.0</td>\n",
       "      <td>-2.596443e-02</td>\n",
       "      <td>0.699231</td>\n",
       "      <td>-0.737183</td>\n",
       "      <td>-0.737162</td>\n",
       "      <td>-0.242332</td>\n",
       "      <td>3.004422</td>\n",
       "      <td>3.012835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_5</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-3.655446e-02</td>\n",
       "      <td>0.483267</td>\n",
       "      <td>-0.254024</td>\n",
       "      <td>-0.254024</td>\n",
       "      <td>-0.209023</td>\n",
       "      <td>2.918075</td>\n",
       "      <td>2.918403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_39</th>\n",
       "      <td>55828.0</td>\n",
       "      <td>-1.726354e-02</td>\n",
       "      <td>0.904190</td>\n",
       "      <td>-0.585441</td>\n",
       "      <td>-0.585441</td>\n",
       "      <td>-0.549304</td>\n",
       "      <td>3.154447</td>\n",
       "      <td>3.154475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_7</th>\n",
       "      <td>45622.0</td>\n",
       "      <td>-8.037879e-03</td>\n",
       "      <td>0.951489</td>\n",
       "      <td>-1.074423</td>\n",
       "      <td>-1.074399</td>\n",
       "      <td>-0.390278</td>\n",
       "      <td>3.793478</td>\n",
       "      <td>3.793688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count          mean       std       min        1%       50%  \\\n",
       "D_62      48189.0 -1.060948e-02  0.944455 -0.820717 -0.820714 -0.430038   \n",
       "D_75      55828.0 -1.089630e-02  0.943937 -0.764539 -0.764539 -0.430422   \n",
       "B_2       55779.0  3.663197e-05  0.999942 -1.558451 -1.558449  0.477259   \n",
       "B_6       55818.0 -3.361485e-02  0.220877 -0.213237 -0.213237 -0.093159   \n",
       "S_3       45622.0 -1.135974e-02  0.922493 -1.139953 -1.139902 -0.332569   \n",
       "D_112     55766.0  2.124820e-05  0.999954 -2.337388 -2.337386  0.428788   \n",
       "B_4       55828.0 -1.155608e-02  0.939662 -0.770369 -0.770365 -0.402848   \n",
       "D_51      55828.0 -7.955249e-03  0.962945 -0.591157 -0.591157 -0.561613   \n",
       "B_23      55828.0 -5.240230e-03  0.978074 -0.742303 -0.742303 -0.483066   \n",
       "D_64_O    55828.0  6.770952e-17  1.000009 -1.032779 -1.032779  0.968262   \n",
       "D_50      23940.0 -3.711052e-02  0.424495 -0.454422 -0.454408 -0.164277   \n",
       "P_2       55155.0  3.242735e-03  0.990101 -2.634398 -2.634383  0.129944   \n",
       "D_41      55779.0 -2.611066e-02  0.775114 -0.287232 -0.287232 -0.257311   \n",
       "D_47      55828.0 -2.314265e-03  0.990833 -1.714800 -1.714797 -0.101906   \n",
       "R_3       55828.0 -2.120974e-02  0.840382 -0.577217 -0.577217 -0.534926   \n",
       "D_43      38392.0 -2.287668e-02  0.810595 -0.697607 -0.697599 -0.304335   \n",
       "R_1       55828.0 -1.686142e-02  0.901345 -0.348424 -0.348424 -0.322585   \n",
       "D_42       9854.0 -2.790335e-02  0.789411 -0.753075 -0.753066 -0.261952   \n",
       "R_26       6402.0 -4.036880e-02  0.574373 -0.340933 -0.340933 -0.191259   \n",
       "D_45      55779.0 -5.634861e-03  0.979098 -0.989321 -0.989315 -0.323737   \n",
       "B_9       55828.0 -1.679233e-02  0.881581 -0.665134 -0.665133 -0.567922   \n",
       "B_3       55779.0 -4.624272e-03  0.980626 -0.550884 -0.550884 -0.510392   \n",
       "D_66_1.0  55828.0  2.278196e-17  1.000009 -0.349239 -0.349239 -0.349239   \n",
       "R_27      54269.0  1.335507e-04  0.999631 -2.776491 -2.776489  0.363459   \n",
       "D_61      49899.0 -4.871603e-03  0.986744 -1.275128 -1.275127 -0.154373   \n",
       "B_7       55828.0 -5.891742e-03  0.975613 -0.798153 -0.798150 -0.472853   \n",
       "D_132      5484.0 -3.794935e-02  0.685445 -0.828160 -0.828113 -0.185995   \n",
       "D_46      42258.0 -1.186074e-03  0.804484 -2.749854 -2.749630 -0.091835   \n",
       "B_1       55828.0 -7.529544e-03  0.961959 -0.583203 -0.583200 -0.432805   \n",
       "D_49       5533.0 -2.596443e-02  0.699231 -0.737183 -0.737162 -0.242332   \n",
       "B_5       55828.0 -3.655446e-02  0.483267 -0.254024 -0.254024 -0.209023   \n",
       "D_39      55828.0 -1.726354e-02  0.904190 -0.585441 -0.585441 -0.549304   \n",
       "S_7       45622.0 -8.037879e-03  0.951489 -1.074423 -1.074399 -0.390278   \n",
       "\n",
       "               99%       max  \n",
       "D_62      3.638186  3.638394  \n",
       "D_75      3.754295  3.754376  \n",
       "B_2       0.967569  0.967569  \n",
       "B_6       1.285047  1.285272  \n",
       "S_3       3.996685  3.997569  \n",
       "D_112     0.444942  0.444942  \n",
       "B_4       3.761493  3.761549  \n",
       "D_51      3.631421  3.631422  \n",
       "B_23      3.703590  3.703861  \n",
       "D_64_O    0.968262  0.968262  \n",
       "D_50      2.244762  2.245851  \n",
       "P_2       1.456492  1.456492  \n",
       "D_41      4.441462  4.442563  \n",
       "D_47      2.538762  2.539341  \n",
       "R_3       3.875588  3.875620  \n",
       "D_43      3.895190  3.895852  \n",
       "R_1       4.223875  4.223893  \n",
       "D_42      3.294024  3.296312  \n",
       "R_26      3.438385  3.439473  \n",
       "D_45      3.114217  3.114469  \n",
       "B_9       2.742118  2.742730  \n",
       "B_3       3.763595  3.763759  \n",
       "D_66_1.0  2.863369  2.863369  \n",
       "R_27      0.380876  0.380876  \n",
       "D_61      1.687814  1.687847  \n",
       "B_7       3.579867  3.579970  \n",
       "D_132     2.977135  2.977399  \n",
       "D_46      3.119942  3.120153  \n",
       "B_1       4.198606  4.198745  \n",
       "D_49      3.004422  3.012835  \n",
       "B_5       2.918075  2.918403  \n",
       "D_39      3.154447  3.154475  \n",
       "S_7       3.793478  3.793688  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_normalized.describe(percentiles=[0.01,0.99]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "108ec652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Value Imputation\n",
    "X_train_normalized.fillna(0,inplace=True)\n",
    "X_test_normalized.fillna(0,inplace=True)\n",
    "X_test1_normalized.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2fa6d50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D_62        0\n",
       "D_75        0\n",
       "B_2         0\n",
       "B_6         0\n",
       "S_3         0\n",
       "D_112       0\n",
       "B_4         0\n",
       "D_51        0\n",
       "B_23        0\n",
       "D_64_O      0\n",
       "D_50        0\n",
       "P_2         0\n",
       "D_41        0\n",
       "D_47        0\n",
       "R_3         0\n",
       "D_43        0\n",
       "R_1         0\n",
       "D_42        0\n",
       "R_26        0\n",
       "D_45        0\n",
       "B_9         0\n",
       "B_3         0\n",
       "D_66_1.0    0\n",
       "R_27        0\n",
       "D_61        0\n",
       "B_7         0\n",
       "D_132       0\n",
       "D_46        0\n",
       "B_1         0\n",
       "D_49        0\n",
       "B_5         0\n",
       "D_39        0\n",
       "S_7         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_normalized.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00f3bfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/qtwpdq7n0g51j49ll8v0y0fm0000gn/T/ipykernel_97404/1480584889.py:27: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  nn_model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=32, verbose=0)\n",
      "2023-04-06 17:19:28.012043: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 0s 248us/step\n",
      "349/349 [==============================] - 0s 250us/step\n",
      "349/349 [==============================] - 0s 243us/step\n",
      "349/349 [==============================] - 0s 290us/step\n",
      "349/349 [==============================] - 0s 238us/step\n",
      "349/349 [==============================] - 0s 240us/step\n",
      "349/349 [==============================] - 0s 240us/step\n",
      "349/349 [==============================] - 0s 242us/step\n",
      "349/349 [==============================] - 0s 238us/step\n",
      "349/349 [==============================] - 0s 237us/step\n",
      "349/349 [==============================] - 0s 239us/step\n",
      "349/349 [==============================] - 0s 252us/step\n",
      "349/349 [==============================] - 0s 247us/step\n",
      "349/349 [==============================] - 0s 250us/step\n",
      "349/349 [==============================] - 0s 245us/step\n",
      "349/349 [==============================] - 0s 245us/step\n",
      "349/349 [==============================] - 0s 240us/step\n",
      "349/349 [==============================] - 0s 251us/step\n",
      "349/349 [==============================] - 0s 244us/step\n",
      "349/349 [==============================] - 0s 240us/step\n",
      "349/349 [==============================] - 0s 245us/step\n",
      "349/349 [==============================] - 0s 239us/step\n",
      "349/349 [==============================] - 0s 240us/step\n",
      "349/349 [==============================] - 0s 238us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 232us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 236us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 246us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 230us/step\n",
      "349/349 [==============================] - 0s 264us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 236us/step\n",
      "349/349 [==============================] - 0s 238us/step\n",
      "349/349 [==============================] - 0s 235us/step\n",
      "349/349 [==============================] - 0s 236us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 228us/step\n",
      "349/349 [==============================] - 0s 230us/step\n",
      "349/349 [==============================] - 0s 227us/step\n",
      "349/349 [==============================] - 0s 228us/step\n",
      "349/349 [==============================] - 0s 225us/step\n",
      "349/349 [==============================] - 0s 226us/step\n",
      "349/349 [==============================] - 0s 228us/step\n",
      "349/349 [==============================] - 0s 247us/step\n",
      "349/349 [==============================] - 0s 230us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 246us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 232us/step\n",
      "349/349 [==============================] - 0s 232us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 230us/step\n",
      "349/349 [==============================] - 0s 235us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 228us/step\n",
      "349/349 [==============================] - 0s 230us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 238us/step\n",
      "349/349 [==============================] - 0s 232us/step\n",
      "349/349 [==============================] - 0s 241us/step\n",
      "349/349 [==============================] - 0s 232us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 247us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 232us/step\n",
      "349/349 [==============================] - 0s 237us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 227us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 223us/step\n",
      "349/349 [==============================] - 0s 226us/step\n",
      "349/349 [==============================] - 0s 224us/step\n",
      "349/349 [==============================] - 0s 227us/step\n",
      "349/349 [==============================] - 0s 225us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 227us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 230us/step\n",
      "349/349 [==============================] - 0s 285us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 230us/step\n",
      "349/349 [==============================] - 0s 243us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 235us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 215us/step\n",
      "349/349 [==============================] - 0s 227us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 228us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 226us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 239us/step\n",
      "349/349 [==============================] - 0s 239us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 237us/step\n",
      "349/349 [==============================] - 0s 235us/step\n",
      "349/349 [==============================] - 0s 235us/step\n",
      "349/349 [==============================] - 0s 223us/step\n",
      "349/349 [==============================] - 0s 237us/step\n",
      "349/349 [==============================] - 0s 227us/step\n",
      "349/349 [==============================] - 0s 226us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 227us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 226us/step\n",
      "349/349 [==============================] - 0s 226us/step\n",
      "349/349 [==============================] - 0s 233us/step\n",
      "349/349 [==============================] - 0s 224us/step\n",
      "349/349 [==============================] - 0s 227us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 230us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 228us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 237us/step\n",
      "349/349 [==============================] - 0s 250us/step\n",
      "349/349 [==============================] - 0s 232us/step\n",
      "349/349 [==============================] - 0s 243us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 234us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 228us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 0s 228us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 229us/step\n",
      "349/349 [==============================] - 0s 231us/step\n",
      "349/349 [==============================] - 0s 238us/step\n",
      "349/349 [==============================] - 0s 550us/step\n",
      "349/349 [==============================] - 0s 238us/step\n",
      "349/349 [==============================] - 0s 239us/step\n",
      "349/349 [==============================] - 0s 237us/step\n",
      "349/349 [==============================] - 0s 238us/step\n",
      "349/349 [==============================] - 0s 243us/step\n",
      "349/349 [==============================] - 0s 260us/step\n",
      "349/349 [==============================] - 0s 239us/step\n",
      "349/349 [==============================] - 0s 241us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "1 fits failed out of a total of 160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/wrappers/scikit_learn.py\", line 248, in fit\n",
      "    return super().fit(x, y, **kwargs)\n",
      "  File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/wrappers/scikit_learn.py\", line 175, in fit\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "  File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\n",
      "\n",
      "Detected at node 'mul_24' defined at (most recent call last):\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "      app.start()\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 711, in start\n",
      "      self.io_loop.start()\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "      self._run_once()\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n",
      "      handle._run()\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "      await self.process_one()\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "      await dispatch(*args)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "      await result\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n",
      "      reply_content = await reply_content\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n",
      "      res = shell.run_cell(\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n",
      "      return super().run_cell(*args, **kwargs)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n",
      "      result = self._run_cell(\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n",
      "      result = runner(coro)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n",
      "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n",
      "      if await self.run_code(code, result, async_=asy):\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"/var/folders/zq/qtwpdq7n0g51j49ll8v0y0fm0000gn/T/ipykernel_97404/1480584889.py\", line 40, in <module>\n",
      "      grid_search.fit(X_train_normalized, y_train)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py\", line 874, in fit\n",
      "      self._run_search(evaluate_candidates)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py\", line 1388, in _run_search\n",
      "      evaluate_candidates(ParameterGrid(self.param_grid))\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py\", line 821, in evaluate_candidates\n",
      "      out = parallel(\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "      return super().__call__(iterable_with_config)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 1051, in __call__\n",
      "      while self.dispatch_one_batch(iterator):\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 864, in dispatch_one_batch\n",
      "      self._dispatch(tasks)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 782, in _dispatch\n",
      "      job = self._backend.apply_async(batch, callback=cb)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "      result = ImmediateResult(func)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "      self.results = batch()\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 263, in __call__\n",
      "      return [func(*args, **kwargs)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 263, in <listcomp>\n",
      "      return [func(*args, **kwargs)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "      return self.function(*args, **kwargs)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "      estimator.fit(X_train, y_train, **fit_params)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/wrappers/scikit_learn.py\", line 248, in fit\n",
      "      return super().fit(x, y, **kwargs)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/wrappers/scikit_learn.py\", line 175, in fit\n",
      "      history = self.model.fit(x, y, **fit_args)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n",
      "      tmp_logs = self.train_function(iterator)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n",
      "      return step_function(self, iterator)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n",
      "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n",
      "      outputs = model.train_step(data)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 1027, in train_step\n",
      "      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n",
      "      self.apply_gradients(grads_and_vars)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n",
      "      return super().apply_gradients(grads_and_vars, name=name)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n",
      "      iteration = self._internal_apply_gradients(grads_and_vars)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n",
      "      return tf.__internal__.distribute.interim.maybe_merge_call(\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n",
      "      distribution.extended.update(\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1213, in apply_grad_to_update_var\n",
      "      return self._update_step(grad, var)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 224, in _update_step\n",
      "      self.update_step(gradient, variable)\n",
      "    File \"/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/adam.py\", line 200, in update_step\n",
      "      variable.assign_sub((m * alpha) / (tf.sqrt(v) + self.epsilon))\n",
      "Node: 'mul_24'\n",
      "Incompatible shapes: [6] vs. [0]\n",
      "\t [[{{node mul_24}}]] [Op:__inference_train_function_1689066]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/sakshisaxena/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.92485475 0.92505246 0.92421111 0.92526558 0.91110775 0.92410828\n",
      " 0.89401523 0.90695331 0.80725298 0.86064446 0.79149303        nan\n",
      " 0.82807951 0.84520524 0.61931133 0.82737121 0.9244986  0.92397801\n",
      " 0.92453354 0.92440822 0.92330027 0.92349125 0.9228375  0.92334495\n",
      " 0.86330038 0.89029703 0.85399703 0.90271925 0.85508609 0.8869718\n",
      " 0.87092391 0.88691584]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC: 0.925265578641231\n",
      "Best parameters: {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n"
     ]
    }
   ],
   "source": [
    "#Neural Network-Grid search\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define the neural network model\n",
    "def create_model(hidden_layers=2, nodes_per_layer=4, activation='relu', dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    for i in range(hidden_layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(nodes_per_layer, activation=activation, input_shape=(X_train_normalized.shape[1],)))\n",
    "        else:\n",
    "            model.add(Dense(nodes_per_layer, activation=activation))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with default parameters\n",
    "nn_model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the hyperparameters for the grid search\n",
    "params = {\n",
    "    'hidden_layers': [2, 4],\n",
    "    'nodes_per_layer': [4, 6],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'dropout_rate': [0,0.5],\n",
    "    'batch_size': [100, 10000]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(nn_model, param_grid=params, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best AUC:\", grid_search.best_score_)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85411d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9248547451469868\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9250524641529125\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9242111120448824\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.925265578641231\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9111077510508782\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9241082809998646\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8940152349979436\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9069533061971715\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8072529777732816\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.8606444566981816\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.7914930256020709\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score nan\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8280795060612508\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.8452052351083903\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.6193113323108217\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.827371208784022\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9244985986817499\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9239780061994803\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9245335424153295\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9244082187956856\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9233002707165452\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9234912484317535\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9228374989163838\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9233449471764377\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8633003796356624\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.8902970290076346\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8539970344172607\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9027192501748177\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8550860898658772\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.8869718019833389\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.870923905299497\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.8869158406321386\n"
     ]
    }
   ],
   "source": [
    "results_nn = grid_search.cv_results_\n",
    "for i in range(len(results_nn['params'])):\n",
    "    print(\"Parameters\",results_nn['params'][i])\n",
    "    print(\"AUC Score\",results_nn['mean_test_score'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "901088dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.924855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.925052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.924211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.925266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.911108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.924108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.894015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.906953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.807253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.860644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.791493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.828080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.845205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.619311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.827371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.924499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.923978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.924534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.924408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.923300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.923491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.922837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.923345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.863300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.890297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.853997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.902719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.855086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.886972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.870924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.886916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  auc_score\n",
       "0   {'activation': 'relu', 'batch_size': 100, 'dro...   0.924855\n",
       "1   {'activation': 'relu', 'batch_size': 100, 'dro...   0.925052\n",
       "2   {'activation': 'relu', 'batch_size': 100, 'dro...   0.924211\n",
       "3   {'activation': 'relu', 'batch_size': 100, 'dro...   0.925266\n",
       "4   {'activation': 'relu', 'batch_size': 100, 'dro...   0.911108\n",
       "5   {'activation': 'relu', 'batch_size': 100, 'dro...   0.924108\n",
       "6   {'activation': 'relu', 'batch_size': 100, 'dro...   0.894015\n",
       "7   {'activation': 'relu', 'batch_size': 100, 'dro...   0.906953\n",
       "8   {'activation': 'relu', 'batch_size': 10000, 'd...   0.807253\n",
       "9   {'activation': 'relu', 'batch_size': 10000, 'd...   0.860644\n",
       "10  {'activation': 'relu', 'batch_size': 10000, 'd...   0.791493\n",
       "11  {'activation': 'relu', 'batch_size': 10000, 'd...        NaN\n",
       "12  {'activation': 'relu', 'batch_size': 10000, 'd...   0.828080\n",
       "13  {'activation': 'relu', 'batch_size': 10000, 'd...   0.845205\n",
       "14  {'activation': 'relu', 'batch_size': 10000, 'd...   0.619311\n",
       "15  {'activation': 'relu', 'batch_size': 10000, 'd...   0.827371\n",
       "16  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.924499\n",
       "17  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.923978\n",
       "18  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.924534\n",
       "19  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.924408\n",
       "20  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.923300\n",
       "21  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.923491\n",
       "22  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.922837\n",
       "23  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.923345\n",
       "24  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.863300\n",
       "25  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.890297\n",
       "26  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.853997\n",
       "27  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.902719\n",
       "28  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.855086\n",
       "29  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.886972\n",
       "30  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.870924\n",
       "31  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.886916"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_nn = pd.DataFrame({\n",
    "    'params': results_nn['params'],\n",
    "    'auc_score': results_nn['mean_test_score']\n",
    "})\n",
    "\n",
    "# Write the dataframe to a CSV file\n",
    "results_nn.to_csv('grid_search_results_nn.csv', index=False)\n",
    "\n",
    "results_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc260c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1745/1745 [==============================] - 0s 222us/step\n",
      "347/347 [==============================] - 0s 225us/step\n",
      "503/503 [==============================] - 0s 217us/step\n",
      "1745/1745 [==============================] - 0s 219us/step\n",
      "347/347 [==============================] - 0s 221us/step\n",
      "503/503 [==============================] - 0s 220us/step\n",
      "1745/1745 [==============================] - 0s 222us/step\n",
      "347/347 [==============================] - 0s 223us/step\n",
      "503/503 [==============================] - 0s 221us/step\n",
      "1745/1745 [==============================] - 0s 219us/step\n",
      "347/347 [==============================] - 0s 222us/step\n",
      "503/503 [==============================] - 0s 220us/step\n",
      "1745/1745 [==============================] - 0s 220us/step\n",
      "347/347 [==============================] - 0s 223us/step\n",
      "503/503 [==============================] - 0s 221us/step\n",
      "1745/1745 [==============================] - 0s 221us/step\n",
      "347/347 [==============================] - 0s 221us/step\n",
      "503/503 [==============================] - 0s 220us/step\n",
      "1745/1745 [==============================] - 0s 221us/step\n",
      "347/347 [==============================] - 0s 223us/step\n",
      "503/503 [==============================] - 0s 222us/step\n",
      "1745/1745 [==============================] - 0s 219us/step\n",
      "347/347 [==============================] - 0s 222us/step\n",
      "503/503 [==============================] - 0s 222us/step\n",
      "1745/1745 [==============================] - 0s 223us/step\n",
      "347/347 [==============================] - 0s 222us/step\n",
      "503/503 [==============================] - 0s 221us/step\n",
      "1745/1745 [==============================] - 0s 218us/step\n",
      "347/347 [==============================] - 0s 220us/step\n",
      "503/503 [==============================] - 0s 226us/step\n",
      "1745/1745 [==============================] - 0s 219us/step\n",
      "347/347 [==============================] - 0s 222us/step\n",
      "503/503 [==============================] - 0s 225us/step\n",
      "1745/1745 [==============================] - 0s 221us/step\n",
      "347/347 [==============================] - 0s 222us/step\n",
      "503/503 [==============================] - 0s 221us/step\n",
      "1745/1745 [==============================] - 0s 214us/step\n",
      "347/347 [==============================] - 0s 237us/step\n",
      "503/503 [==============================] - 0s 240us/step\n",
      "1745/1745 [==============================] - 0s 221us/step\n",
      "347/347 [==============================] - 0s 219us/step\n",
      "503/503 [==============================] - 0s 219us/step\n",
      "1745/1745 [==============================] - 0s 226us/step\n",
      "347/347 [==============================] - 0s 221us/step\n",
      "503/503 [==============================] - 0s 222us/step\n",
      "1745/1745 [==============================] - 0s 220us/step\n",
      "347/347 [==============================] - 0s 219us/step\n",
      "503/503 [==============================] - 0s 220us/step\n",
      "1745/1745 [==============================] - 0s 220us/step\n",
      "347/347 [==============================] - 0s 231us/step\n",
      "503/503 [==============================] - 0s 221us/step\n",
      "1745/1745 [==============================] - 0s 222us/step\n",
      "347/347 [==============================] - 0s 226us/step\n",
      "503/503 [==============================] - 0s 221us/step\n",
      "1745/1745 [==============================] - 0s 227us/step\n",
      "347/347 [==============================] - 0s 221us/step\n",
      "503/503 [==============================] - 0s 221us/step\n",
      "1745/1745 [==============================] - 0s 221us/step\n",
      "347/347 [==============================] - 0s 222us/step\n",
      "503/503 [==============================] - 0s 222us/step\n",
      "1745/1745 [==============================] - 0s 219us/step\n",
      "347/347 [==============================] - 0s 218us/step\n",
      "503/503 [==============================] - 0s 221us/step\n",
      "1745/1745 [==============================] - 0s 220us/step\n",
      "347/347 [==============================] - 0s 222us/step\n",
      "503/503 [==============================] - 0s 221us/step\n",
      "1745/1745 [==============================] - 0s 233us/step\n",
      "347/347 [==============================] - 0s 223us/step\n",
      "503/503 [==============================] - 0s 233us/step\n",
      "1745/1745 [==============================] - 0s 222us/step\n",
      "347/347 [==============================] - 0s 223us/step\n",
      "503/503 [==============================] - 0s 222us/step\n",
      "1745/1745 [==============================] - 0s 219us/step\n",
      "347/347 [==============================] - 0s 222us/step\n",
      "503/503 [==============================] - 0s 220us/step\n",
      "1745/1745 [==============================] - 0s 220us/step\n",
      "347/347 [==============================] - 0s 220us/step\n",
      "503/503 [==============================] - 0s 219us/step\n",
      "1745/1745 [==============================] - 0s 231us/step\n",
      "347/347 [==============================] - 0s 222us/step\n",
      "503/503 [==============================] - 0s 228us/step\n",
      "1745/1745 [==============================] - 0s 224us/step\n",
      "347/347 [==============================] - 0s 221us/step\n",
      "503/503 [==============================] - 0s 224us/step\n",
      "1745/1745 [==============================] - 0s 242us/step\n",
      "347/347 [==============================] - 0s 240us/step\n",
      "503/503 [==============================] - 0s 239us/step\n",
      "1745/1745 [==============================] - 0s 240us/step\n",
      "347/347 [==============================] - 0s 237us/step\n",
      "503/503 [==============================] - 0s 243us/step\n",
      "1745/1745 [==============================] - 0s 229us/step\n",
      "347/347 [==============================] - 0s 268us/step\n",
      "503/503 [==============================] - 0s 268us/step\n",
      "1745/1745 [==============================] - 0s 236us/step\n",
      "347/347 [==============================] - 0s 243us/step\n",
      "503/503 [==============================] - 0s 240us/step\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters for the grid search\n",
    "\n",
    "# Create empty lists to store results\n",
    "results_train = []\n",
    "results_test_1 = []\n",
    "results_test_2 = []\n",
    "h= []\n",
    "nodes=[]\n",
    "act=[]\n",
    "drop=[]\n",
    "batch=[]\n",
    "scores_nn = pd.DataFrame()\n",
    "\n",
    "# Nested for loop to iterate through hyperparameters\n",
    "for hidden_layers in [2,4]:\n",
    "    for nodes_per_layer in [4,6]:\n",
    "        for activation in ['relu','tanh']:\n",
    "            for dropout_rate in [0,0.5]:\n",
    "                for batch_size in [100,10000]:\n",
    "                    # Create the model with the current hyperparameters\n",
    "                    model_nn_test = Sequential()\n",
    "                    for i in range(hidden_layers):\n",
    "                        if i == 0:\n",
    "                            model_nn_test.add(Dense(nodes_per_layer, activation=activation, input_shape=(X_train_normalized.shape[1],)))\n",
    "                        else:\n",
    "                            model_nn_test.add(Dense(nodes_per_layer, activation=activation))\n",
    "                        if dropout_rate > 0:\n",
    "                            model_nn_test.add(Dropout(dropout_rate))\n",
    "                    model_nn_test.add(Dense(1, activation='sigmoid'))\n",
    "                    model_nn_test.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    # Train the model on the training data\n",
    "                    model_nn_test.fit(X_train_normalized, y_train, epochs=20, batch_size=batch_size, verbose=0)\n",
    "                    \n",
    "                    #creating lists\n",
    "                    h.append(hidden_layers)\n",
    "                    nodes.append(nodes_per_layer)\n",
    "                    act.append(activation)\n",
    "                    drop.append(dropout_rate)\n",
    "                    batch.append(batch_size)\n",
    "                    # Calculate AUC scores for the training data and the two test datasets\n",
    "                    y_train_pred = model_nn_test.predict(X_train_normalized)\n",
    "                    auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "                    \n",
    "                    y_test_1_pred = model_nn_test.predict(X_test_normalized)\n",
    "                    auc_test_1 = roc_auc_score(y_test, y_test_1_pred)\n",
    "                    \n",
    "                    y_test_2_pred = model_nn_test.predict(X_test1_normalized)\n",
    "                    auc_test_2 = roc_auc_score(y_test1, y_test_2_pred)\n",
    "                    \n",
    "                    # Store the results in the lists\n",
    "                    results_train.append(auc_train)\n",
    "                    results_test_1.append(auc_test_1)\n",
    "                    results_test_2.append(auc_test_2)\n",
    "\n",
    "scores_nn['hidden_layers'] = h\n",
    "scores_nn['nodes_per_layer'] = nodes\n",
    "scores_nn['activation'] = act\n",
    "scores_nn['dropout_rate'] = drop\n",
    "scores_nn['batch_size'] = batch\n",
    "scores_nn['AUC Train'] = results_train\n",
    "scores_nn['AUC Test 1'] = results_test_1\n",
    "scores_nn['AUC Test 2'] = results_test_2\n",
    "scores_nn.to_csv(\"Neural_Network_Scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0f05ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9255899704664255,\n",
       " 0.8842717183244699,\n",
       " 0.9249654396868535,\n",
       " 0.8614746514053726,\n",
       " 0.9267695791266557,\n",
       " 0.8868292686302737,\n",
       " 0.9238902330018186,\n",
       " 0.8782407559693234,\n",
       " 0.9271166034844234,\n",
       " 0.8837360481665727,\n",
       " 0.9252732683824993,\n",
       " 0.8754148652967213,\n",
       " 0.9273717763500257,\n",
       " 0.9032755084752906,\n",
       " 0.9239865920618326,\n",
       " 0.8741531290171061,\n",
       " 0.9271400018138667,\n",
       " 0.8938029980562511,\n",
       " 0.8692815854872125,\n",
       " 0.7921886917596448,\n",
       " 0.9258701392382371,\n",
       " 0.8850917671524525,\n",
       " 0.9240694567677842,\n",
       " 0.8896030988119823,\n",
       " 0.9268955242077531,\n",
       " 0.8999751375031236,\n",
       " 0.9243046148981587,\n",
       " 0.85338579838803,\n",
       " 0.9273725592960503,\n",
       " 0.9000463855913785,\n",
       " 0.9239259233337085,\n",
       " 0.9043607000151145]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "54289b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9100280584055195,\n",
       " 0.8649500429110107,\n",
       " 0.9105074862793174,\n",
       " 0.8486837449200986,\n",
       " 0.9114713192586916,\n",
       " 0.8671371840077132,\n",
       " 0.9096564023723259,\n",
       " 0.8632513296140608,\n",
       " 0.9127168317387935,\n",
       " 0.8680541991904521,\n",
       " 0.9105027363183967,\n",
       " 0.8679209957408988,\n",
       " 0.9119970422061526,\n",
       " 0.885954211013082,\n",
       " 0.9096764703890391,\n",
       " 0.8551874868665854,\n",
       " 0.9127758312533883,\n",
       " 0.8802690759680569,\n",
       " 0.8696025500881106,\n",
       " 0.7771084246988695,\n",
       " 0.9106586895807811,\n",
       " 0.8669869125167652,\n",
       " 0.9092747464214044,\n",
       " 0.8705089517218063,\n",
       " 0.911724726264752,\n",
       " 0.8820282433130892,\n",
       " 0.9101302848372019,\n",
       " 0.8267991976975099,\n",
       " 0.9109707324680646,\n",
       " 0.8834334590247236,\n",
       " 0.9091392929903624,\n",
       " 0.8887997557838274]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e778d99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9375757139078618,\n",
       " 0.8979341528601367,\n",
       " 0.9375963370798859,\n",
       " 0.8718995446342426,\n",
       " 0.9371736050780923,\n",
       " 0.8962755889870846,\n",
       " 0.9375727117400289,\n",
       " 0.8865135526652136,\n",
       " 0.9381760327419101,\n",
       " 0.9038591796654798,\n",
       " 0.9377794788787359,\n",
       " 0.8792398641077334,\n",
       " 0.9379230666383352,\n",
       " 0.9134194875188603,\n",
       " 0.9375298400184918,\n",
       " 0.8864064211665912,\n",
       " 0.9386863725903682,\n",
       " 0.9007779419982702,\n",
       " 0.8574775797341356,\n",
       " 0.794326195363796,\n",
       " 0.9369954350221502,\n",
       " 0.8994146996539514,\n",
       " 0.9375255471097117,\n",
       " 0.9026779317947243,\n",
       " 0.9380552098282026,\n",
       " 0.9164565722846291,\n",
       " 0.9375025432377184,\n",
       " 0.8726837032233568,\n",
       " 0.9375748151697207,\n",
       " 0.9118860012241197,\n",
       " 0.9373385235269758,\n",
       " 0.9137483874343098]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "922a71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC: 0.925265578641231\n",
      "Best parameters: {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Best AUC:\", grid_search.best_score_)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "507f611d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1569fcaf0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best Neural Network\n",
    "# Define the neural network model with the best parameters\n",
    "model_best_nn = Sequential()\n",
    "model_best_nn.add(Dense(6, activation='relu', input_shape=(X_train_normalized.shape[1],)))\n",
    "model_best_nn.add(Dropout(0))\n",
    "model_best_nn.add(Dense(6, activation='relu'))\n",
    "model_best_nn.add(Dropout(0))\n",
    "model_best_nn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_best_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model_best_nn.fit(X_train_normalized, y_train, batch_size=100, epochs=20, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5c0c513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 [==============================] - 0s 249us/step\n",
      "347/347 [==============================] - 0s 292us/step - loss: 0.3119 - accuracy: 0.8535\n",
      "Test loss: 0.312\n",
      "Test accuracy: 0.854\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred_nn = model_best_nn.predict(X_test_normalized)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model_best_nn.evaluate(X_test_normalized, y_test)\n",
    "print(f\"Test loss: {loss:.3f}\")\n",
    "print(f\"Test accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d4975a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default rate for train data is :  0.25999498459554343\n",
      "Default rate for train data is :  0.2338025627143115\n",
      "Default rate for train data is :  0.2823529411764706\n"
     ]
    }
   ],
   "source": [
    "###Strategy#####\n",
    "Default_rate_train = sum(y_train['target'])/(len(y_train['target']))\n",
    "Default_rate_test= sum(y_test['target'])/(len(y_test['target']))\n",
    "Default_rate_test1=sum(y_test1['target'])/(len(y_test1['target']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Default rate for train data is : \",Default_rate_train)\n",
    "print(\"Default rate for train data is : \",Default_rate_test)\n",
    "print(\"Default rate for train data is : \",Default_rate_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2c4701a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['target'], dtype='object')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6d5ece18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_test1 = pd.DataFrame(y_test1[['target']])\n",
    "\n",
    "Data_test1.to_csv(\"Probability_of_default.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0f295c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16065"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "75cd71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_test1=Data_test1.sort_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "36d1b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb = pd.DataFrame(xgb_best_model.predict_proba(X_test1_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9e83ca82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16065"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Data_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4c24e0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16065, 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c26c7622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16065, 2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7e3eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_test1 = Data_test1.reset_index(drop=True)\n",
    "df_xgb = df_xgb.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c9558b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([Data_test1, df_xgb],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3735ce00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16065, 3)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_new)\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4508442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_testing_rate=df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "212fb519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "\n",
    "fpr,tpr,threshold= roc_curve(nm.array(y_test1),xgb_best_model.predict(X_test1_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "79e4a021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youden_index = tpr-fpr\n",
    "type(youden_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "07412c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm.argmax(youden_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c17d6927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.11267239 1.        ] [0.         0.82848325 1.        ] [2 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(fpr,tpr,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b689c116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold[nm.argmax(youden_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cb82757b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9        1\n",
       "10       1\n",
       "12       1\n",
       "14       0\n",
       "20       1\n",
       "        ..\n",
       "82938    1\n",
       "82942    0\n",
       "82945    0\n",
       "82950    0\n",
       "82953    0\n",
       "Name: target, Length: 16065, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b617743e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/sakshisaxena/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/Spring'23/ML Project\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "58f23b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.19443856180286\n",
      "0.09921119875569381\n"
     ]
    }
   ],
   "source": [
    "#Strategy\n",
    "\n",
    "#def revenue(TH,x,y,TR):\n",
    "def revenue(TH,x,y):\n",
    "    df = pd.DataFrame(y)\n",
    "    df['Prob of 1'] = xgb_best_model.predict_proba(x)[:, 1]\n",
    "    df['Accepted_customers'] = (df['Prob of 1'] < TH).astype(int)\n",
    "    df = df[df['Accepted_customers'] == 1]\n",
    "    balance_feature = x['B_9']\n",
    "    spend_feature = x['S_3']\n",
    "    df['B9_Balance'] = balance_feature\n",
    "    df['S3_Spend'] = spend_feature\n",
    "    df = df.dropna()\n",
    "    RR = nm.mean(df['target'])\n",
    "    df = df.drop(df[df['target'] == 1].index)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return((sum(df['B9_Balance'])*0.02) + (sum(df['S3_Spend'])*0.001),RR)\n",
    "\n",
    "\n",
    "a,b = revenue(0.56734,X_test1_selected,y_test1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c979e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = pd.concat([X_train_selected,X_test_selected,X_test1_selected])\n",
    "input_y = pd.concat([y_train,y_test,y_test1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9eedae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x103e11750>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/shap/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x103e11a50>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/shap/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x103e11cf0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/shap/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x103e11ea0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/shap/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x103e12050>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/shap/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement shap (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for shap\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b2c99b80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m      2\u001b[0m shap\u001b[38;5;241m.\u001b[39minitjs()\n\u001b[1;32m      3\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(xgb_model)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "explainer = shap.Explainer(xgb_model)\n",
    "shap_values = explainer(X_test1)\n",
    "shap.plots.beeswarm(shap_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c88fbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a1fa85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1e0db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
